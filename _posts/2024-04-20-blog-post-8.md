---
title: 'Fundamental 2: Gaussian Mixture Model and EM algorithm'
date: 2024-04-20
permalink: /posts/2024/04/blog-post-8/
tags:
  - Fundamental Knowledge
published: true
---

Chuyện là mình đang viết cái phần "Paper Explained 6" và nó có nhiều kiến thức bên ngoài quá, nên mình cho thêm 1 blog. Blog này sẽ giải thích về Gaussian Mixture Model và EM algorithm (EM là viết tắc của Expectation-maximization). Blog này sẽ rất là liên quan đến một bài giảng của viện công nghệ IIT, series này rất hay, mọi người có thể xem tập [này](https://www.youtube.com/watch?v=3k0k1BbFUKw&list=PLwdnzlV3ogoVDlDwuB9SLJzhaZT0tTil3&index=31) ở đây.

# Giới thiệu:
Trong rất nhiều lĩnh vực khác nhau như xử lý ảnh (image processing), xử lý tín hiệu số (signal processing), khoa học dữ liệu (data science) và nhiều nhiều lĩnh vực khác nữa, chúng ta thường sẽ giả định phân phối của dữ liệu tuân theo một phân phối Gaussian (Normal distribution - phân phối chuẩn). Dưới đây là hình vẽ của một phân phối chuẩn: 

![gaus](/assets/img/fund2/Gaussian.png){: .align-center}

Theo mình tìm hiểu, có 2 nguyên do chính cho việc này:
1. Phân phối Gaussian là phân phối gần giống với phân phối trong tự nhiên nhất. 
2. Công thức toán dễ dàng biến đổi. 

Trong nhiều trường hợp, như hình vẽ ở trên, dữ liệu của chúng ta khá đơn giản, rất có khả năng dữ liệu của chúng ta sẽ có phân phối Gaussian. Nhưng cũng có nhiều trường hợp mà phân phối của dữ liệu trở nên phức tạp hơn, điều đó khiến cho việc chúng ta ước lượng phân phối của dữ liệu thực tế là một phân phối Gaussian (hoặc gần giống) thì sẽ không hiệu quả (trong trường hợp này mình đang nói đến unimodal Gaussian distribution với một giá trị $\mu$ và một giá trị $\sigma$). Lúc này, công thức toán của phân phối chuẩn sẽ là như sau: 

$$
\begin{equation}
P(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
\end{equation}
$$

Trong nhiều sách khác, người ta sẽ sử dụng một ký hiệu chung như là: $\mathcal{N}(x\|\mu, \sigma^2)$ Tuy nhiên như mình mới nói, do nó không hiệu quả, chúng ta phải cân nhắc **thay vì chỉ dùng một phân phối Gaussian để ước lượng, chúng ta có thể sử dụng nhiều phân phối Gaussian (mixture of Gaussians)**. Cho mọi người dễ hình dung, một cái multimodal Gaussians distribution sẽ nhìn như sau: 

![multi-modal](/assets/img/fund2/multimodal.jpg){: .align-center}

Phần trên là mình đang đề cập về việc đầu vào là biến 1 chiều (univariate variable). Nhưng thông thường chúng ta ít khi làm việc với các giá trị đơn biến (mình thường chỉ gặp lúc làm bài tập về nhà hoặc đọc hiểu tài liệu thôi), thay vào đó, chúng ta thường sẽ làm quen với các dữ liệu nhiều chiều (tức là một biến sẽ có nhiều chiều - multivariate variable). Lúc này, cái phân phối chuẩn ở trên sẽ được biểu diễn khác đi, sang một công thức tổng quát hơn cho phân phối Gauss (multivariate Gaussian distribution):

$$
\begin{equation}
P(\mathbf{x}) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{x}-\mathbf{\mu})}
\end{equation}
$$

Có thể coi multivaritate Gaussian distribution là một dạng tổng quát hơn công thức phân phối chuẩn đơn biến mà mọi người thường gặp. Lưu ý là trong trường hợp sử dụng phân phối Gaussian đa biến, cũng sẽ có trường hợp unimodal, bimodal, multimodal distribution như trường hợp ở trên, nên là đừng nhầm lẫn. Mọi người có thể quan sát hình dưới đây: 

![multivariate_multimodal](/assets/img/fund2/multivariate_multimodal.jpg){: .align-center}

Trong trường hợp của unimodal Gaussian Distribution, chúng ta thường sẽ có 2 tham số, một giá trị $\mu$ để đại diện cho giá trị trung bình và một giá trị $\sigma$ để đại diện cho độ lệch chuẩn, nhưng qua tới trường hợp multimodal Gaussian Distribution, do lúc này chúng ta có nhiều "đỉnh" khác nhau, dẫn tới thay vì giá trị $\mu$ với $\sigma$ đang là dạng số vô hướng, thì bây giờ chúng sẽ được biểu diễn bằng các vector $\mathbf{\mu}$ cho các giá trị trung bình, $\mathbf{\sigma}$ cho các giá trị về độ lệch chuẩn. Còn trong trường hợp biến nhiều chiều, lúc này ...

Trong trường hợp chúng ta chỉ có một phân phối Gaussian (unimodal) thì chúng ta có thể xấp xỉ các tham số trong mô hình xác suất bằng phương pháp như ước lượng hợp lý cực đại (Maximum likelihood estimation), nhưng khi gặp phải trường hợp nhiều phân phối Gaussian khác nhau, chúng ta khó có thể làm được như vậy, bởi vì phương pháp MLE này không hoạt động cho multimodal Gaussian Distribution. Do đó chúng ta cần một phương pháp tiếp cận khác là EM (Expectation-maximization). 

Lý do cho việc MLE không phải là một phương pháp ước lượng tốt cho mô hình xác suất multimodal Gaussian là bởi vì:
- Objective của MLE là tìm được một bộ tham số tối ưu (vector $\mathbf{\mu}$ và ma trận hiệp phương sai $\mathbf{\Sigma}$) để có thể tối đa khả năng mà dữ liệu được sinh ra từ mô hình xác suất đó. 
- Trong trường hợp của unimodal Gaussian distribution, các tham số mà chúng ta mong muốn tìm được trong quá trình ước lượng là bằng đúng với $\mathbf{mu}$ và $\mathbf{\Sigma}$ được sinh ra từ dữ liệu. Nhưng trong trường hợp multimodal Gaussian distribution thì không dễ như vậy, do có nhiều phân phối Gaussian kết hợp lại với nhau, khiến cho việc ước lượng sử dụng MLE không hiệu quả nữa. 

# GMM (Gaussian Mixture Modal)

# EM (Expectation-Maximization)