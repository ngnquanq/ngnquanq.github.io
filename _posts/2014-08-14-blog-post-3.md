---
title: 'Paper Explained 3: Masked Autoencoders Are Scalable Vision Learners'
date: 2024-02-28
permalink: /posts/2024/02/blog-post-3/
tags:
  - Paper explained
  - Deep Learning
published: True

---

"Mask" aka 👺 là một kĩ thuật được ưa chuộng trong lĩnh vực NLP. Nó hoạt động bằng cách che đi một phần trong dữ liệu, rồi đoán cái phần bị che đó dựa vào những cái không bị che 🤨👌. Mục tiêu của nó đơn giản là để mô hình học được các biểu diễn chung trong dữ liệu, bất kể ngữ cảnh cụ thể. Vậy còn đối với dữ liệu 🖼️ thì cái này xài sao, giờ mình tìm hiểu trong bài [này](https://arxiv.org/pdf/2111.06377.pdf) nhế !!! 💪.

# Giới thiệu 
Ngày nay mấy mô hình Deep Learning đã trở nên quá là mạnh cũng như là cái khả năng của nó là rất lớn, kết hợp với việc phần cứng giờ mạnh hơn và chuyên dụng hơn cho các bài toán liên quan đến AI làm cho việc con người train mô hình như là diều gặp gió 🪁🍃. 

Bên lề xíu thì đây là dự đoán cho thị trường phần cứng cho AI từ năm 2022 cho tới năm 2030.

![hardware](https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg)

Đó, nói chung là rất khủng. Nhưng mà ae thử nghỉ, giờ cái gì cũng to ra, kiểu như ae đang tuổi ăn tuổi lớn đi, khẩu phần ăn của ae phải nhiều hơn đúng kh ? 🤔 (rõ ràng luôn). Thì mấy cái model này cũng v, giờ mô hình lớn hơn cần nhiều data hơn, kiểu kiểu vậy đó. Trước đó train 1M ảnh còn khó khăn chứ giờ ae train cả chục M ảnh trong phòng cũng daijoubu thoi. Nhưng mà vấn đề nè ae: **🥹 ? Đào đâu ra data giờ ní ? 🥹**

Khó trả lời liền đúng kh ae =)))))))))))). Để cho ae có cái nhìn cụ thể hơn về cái thị trường cũng gọi là 🤑🫰💵💶💷 thì dưới đây là cái hình (nói chung từ năm 2019 lận) dự đoán tăng trưởng tới 2025.

![labeling_cost](https://assets-global.website-files.com/62cd5ce03261cb3e98188470/62cd5ce03261cb756e1885e6_1*8aZc2rNfDVjvZ2Qe1Af52g.png)

Đó, nói chung là hao tiền. Nhưng mà có cách nào khác để counter không? Câu trả lời là YES. Cái sự háu ăn của các model ngày nay có thể được giải quyết bằng phương pháp ✨self-supervised learning✨. 

Cụ thể hơn thì có một cái task cụ thể trong NLP, đó là Masking Task, thì cái này đơn giản là nó huấn luyện mô hình bằng cách dự đoán các từ bị thiếu trong câu 👺🔮. Cụ thể hơn thì đầu tiên nó tiến hành che vài từ trong câu (đương nhiên là ít từ thôi nhé, chứ che nhiều quá thì không dự đoán được👽👌) và sử dụng các từ còn lại để đoán ra từ đó. Như cái hình ở dưới này là một cái ví dụ cụ thể:

![MLM](/assets/img/blog3/mlm.png) 

Thì cái lợi ích của cái việc này là nó cho phép mô hình học các từ dựa trên ngữ cảnh (đương nhiên là không cần label 🥂). Câu hỏi là đối với hình ảnh, mình làm vậy được không? Nói luôn là được nhé! Giờ mình làm cái này nè ae, paper gốc mọi người có thể đọc ở [đây](https://arxiv.org/pdf/2111.06377.pdf) nha! 

Trong cái bài nghiên cứu này, các tác giả đặt ra câu hỏi: **Điều gì làm cho masked autoencoder khác biệt giữa dữ liệu dạng chữ và dữ liệu dạng ảnh?**. Và họ trả lời câu hỏi này bằng việc đặt ra 3 vấn đề chính:

- **Khác biệt trong cấu trúc mô hình🏗️**: Thông thường, khi sử dụng dữ liệu cho dạng ảnh thì người ta dùng mấy cấu trúc CNN, còn cho dữ liệu văn bản thì người ta dùng RNN hoặc là Transformer. Cái vấn đề mấu chốt khiến cho ae không áp dụng cái masking thông thường cho mấy mạng CNN là bởi vì cái phép tích chập nó chỉ hoạt động ok với những cái grid thông thường chứ nó không có xác định được cái nào bị mask và phải deal sao với cái cell bị mask trong cái grid đó. ae thấy vấn đề chưa? Nhưng mà thanks to cấu trúc Transformer, cụ thể là mô hình ViT, mà cái này không còn là chướng ngại nữa. Nên tạm thời một vấn đề được giải quyết.

- **Khác biệt trong mật độ thông tin🤨**: Thì cái mật độ thông tin trong từng loại dữ liệu là khác nhau. ae tưởng tượng ngôn ngữ là do con người quy định, do đó trong văn bản, nó mang tính ngữ nghĩa cao cũng như mật độ thông tin rất dày đặc, hầu hết mọi câu ae thốt ra đều dính dính với nhau, chứ không phải kiểu câu dài 4 chữ mà 4 chữ chả liên quan gì nhau =))))))))))) đọc nó cứ cấn cấn. Hình ảnh thì ngược lại, hình ảnh tồn tại trong tự nhiên, con người không có 'quy ra hình ảnh' được, ngoài ra hình ảnh còn là loại dữ liệu có tương quan không gian cao (này cụm gốc là heavy spatial redundancy và đề cập đến mối liên hệ mật thiết giữa các điểm ảnh trong không gian). Do đó mà đối với hình ảnh, bị hụt một vài chỗ thôi thì vẫn dùng các dữ liệu lân cận để tái cấu trúc lại phần bị khuyết đó chỉ cần mô hình hiểu được một vài bản chất cơ bản của hình ảnh như là đối tượng trong ảnh, khung cảnh trong ảnh, v.v... Mà như vậy thì nếu như chỉ che một xíu như bên dữ liệu văn bản thôi thì dễ quá 🤨👌👎. Nên nhóm tác giả có đề xuất mình sẽ mask phần lớn ảnh, có khi mask tới 85% ảnh luôn, không vấn đề gì cạ! 💯. Có 2 mục đích chính: **Giảm thiểu dư thừa trong tính toán** (chứ che xíu thì dễ quá, tính chi nữa, với cả làm vậy thì phải tính toán rất nhiều mà hiệu quả mang lại rất ít) và **Tạo ra một task đủ khó để hiểu nhiều hơn về tấm ảnh chứ không chỉ đơn giản là vài bản chất cơ bản của tấm ảnh** (điều này ép cho mô hình phải học những cái khó hơn, từ đó làm tốt hơn). 

- **Phần decoder**: Có một sự đối lập trong phần decoder khi dùng để dự đoán cái mask cho dữ liệu ảnh và dữ liệu văn bản. Hiểu đơn giản thì đối với dữ liệu ảnh, phần này sẽ tái cấu trúc lại ở pixel-level, như vậy có thể thấy rằng các cái pixel được tái cấu trúc này có tính ngữ nghĩa thấp. Trong khi đó đối với dữ liệu văn bản thì khác, như nãy mình nói là văn bản là loại dữ liệu có tính ngữ nghĩa dày đặc, thì cái này từ được dữ đoán cũng phải có tính ngữ nghĩa cao. 

Và từ những nhận xét trên mà nhóm tác giả đã đề xuất ra mô hình MAE (Masked AutoEncoder) để học các đặt trưng của ảnh. Với mô tả ngắn gọn (chi tiết vô sau) về mô hình. MAE sẽ che mấy cái patches từ cái ảnh đầu vào rồi tái cấu trúc nó ở pixel-level. Và không như những cái AE mà ae hay gặp, cái AE mà tác giả giới thiệu là một cấu trúc bất đối xứng (bất đối xứng trong AE có nghĩa là phần encoder và decoder có cái kích thước khác nhau). Trong khi phần encoder chỉ hoạt động đối với những phần không bị che thì phần decoder hoạt động luôn cả phần bị che lẫn khôgn bị che. Nói tóm gọn thì là như vậy, mình sẽ giải thích cụ thể hơn ở phần sau, trước mắt mọi người có thể xem cái hình này để nắm được cái mô hình này trông ra sao: 

![MAE](/assets/img/blog3/MAE.png)

Và khi mình nói tới việc tái cấu trúc lại hình thì là như sau:

![Reconstructed](/assets/img/blog3/reconstruct.png)

**REFERENCES**

(1) [https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg](https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg)

(2) [https://www.lightly.ai/post/ai-human-bottleneck](https://www.lightly.ai/post/ai-human-bottleneck)

(3) [https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5]([https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5])

(4) [https://arxiv.org/pdf/2111.06377](https://arxiv.org/pdf/2111.06377)

# Các khái niệm liên quan

## Masked Language Modeling

# Tổng quan mô hình

# Ứng dụng mô hình

# Thảo luận thêm và nhận xét