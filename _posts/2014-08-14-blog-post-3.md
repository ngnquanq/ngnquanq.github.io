---
title: 'Paper Explained 3: Masked Autoencoders Are Scalable Vision Learners'
date: 2024-02-28
permalink: /posts/2024/02/blog-post-3/
tags:
  - Paper explained
  - Deep Learning
published: True

---

"Mask" aka 👺 là một kĩ thuật được ưa chuộng trong lĩnh vực NLP. Nó hoạt động bằng cách che đi một phần trong dữ liệu, rồi đoán cái phần bị che đó dựa vào những cái không bị che 🤨👌. Mục tiêu của nó đơn giản là để mô hình học được các biểu diễn chung trong dữ liệu, bất kể ngữ cảnh cụ thể. Vậy còn đối với dữ liệu 🖼️ thì cái này xài sao, giờ mình tìm hiểu trong bài [này](https://arxiv.org/pdf/2111.06377.pdf) nhế !!! 💪.

# Giới thiệu 
Ngày nay mấy mô hình Deep Learning đã trở nên quá là mạnh cũng như là cái khả năng của nó là rất lớn, kết hợp với việc phần cứng giờ mạnh hơn và chuyên dụng hơn cho các bài toán liên quan đến AI làm cho việc con người train mô hình như là diều gặp gió 🪁🍃. 

Bên lề xíu thì đây là dự đoán cho thị trường phần cứng cho AI từ năm 2022 cho tới năm 2030.

![hardware](https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg)

Đó, nói chung là rất khủng. Nhưng mà ae thử nghỉ, giờ cái gì cũng to ra, kiểu như ae đang tuổi ăn tuổi lớn đi, khẩu phần ăn của ae phải nhiều hơn đúng kh ? 🤔 (rõ ràng luôn). Thì mấy cái model này cũng v, giờ mô hình lớn hơn cần nhiều data hơn, kiểu kiểu vậy đó. Trước đó train 1M ảnh còn khó khăn chứ giờ ae train cả chục M ảnh trong phòng cũng daijoubu thoi. Nhưng mà vấn đề nè ae: **🥹 ? Đào đâu ra data giờ ní ? 🥹**

Khó trả lời liền đúng kh ae =)))))))))))). Để cho ae có cái nhìn cụ thể hơn về cái thị trường cũng gọi là 🤑🫰💵💶💷 thì dưới đây là cái hình (nói chung từ năm 2019 lận) dự đoán tăng trưởng tới 2025.

![labeling_cost](https://assets-global.website-files.com/62cd5ce03261cb3e98188470/62cd5ce03261cb756e1885e6_1*8aZc2rNfDVjvZ2Qe1Af52g.png)

Đó, nói chung là hao tiền. Nhưng mà có cách nào khác để counter không? Câu trả lời là YES. Cái sự háu ăn của các model ngày nay có thể được giải quyết bằng phương pháp ✨self-supervised learning✨. 

Cụ thể hơn thì có một cái task cụ thể trong NLP, đó là Masking Task, thì cái này đơn giản là nó huấn luyện mô hình bằng cách dự đoán các từ bị thiếu trong câu 👺🔮. Cụ thể hơn thì đầu tiên nó tiến hành che vài từ trong câu (đương nhiên là ít từ thôi nhé, chứ che nhiều quá thì không dự đoán được👽👌) và sử dụng các từ còn lại để đoán ra từ đó. Như cái hình ở dưới này là một cái ví dụ cụ thể:

![MLM](/assets/img/blog3/mlm.png) 

Thì cái lợi ích của cái việc này là nó cho phép mô hình học các từ dựa trên ngữ cảnh (đương nhiên là không cần label 🥂). Câu hỏi là đối với hình ảnh, mình làm vậy được không? Nói luôn là được nhé! Giờ mình làm cái này nè ae, paper gốc mọi người có thể đọc ở [đây](https://arxiv.org/pdf/2111.06377.pdf) nha! 

## Reference:
(1) [https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg](https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg)

(2) [https://www.lightly.ai/post/ai-human-bottleneck](https://www.lightly.ai/post/ai-human-bottleneck)

(3) [https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5]

(4) [https://arxiv.org/pdf/2111.06377](https://arxiv.org/pdf/2111.06377)

# Các khái niệm liên quan

# Tổng quan mô hình

# Ứng dụng mô hình

# Thảo luận thêm và nhận xét