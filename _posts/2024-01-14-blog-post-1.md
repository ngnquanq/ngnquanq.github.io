---
title: 'Paper Explained 1: Convolutional Neural Network for Sentence Classification'
date: 2024-01-14
permalink: /posts/2024/01/blog-post-1/
tags:
  - Paper explained
  - Deep Learning
---

Chào mọi người, ở trong số đầu tiên, chúng ta sẽ tìm hiểu về cách thức mà một mạng CNN (vốn rất nổi tiếng trong các bài toán xử lí ảnh) lại có thể áp dụng được vào trong bài toán xử lí ngôn ngữ tự nhiên, mà ở đây cụ thể là bài toán phân loại câu mà vẫn cho ra một kết quả tốt. Và cũng như trong bài này, chúng ta sẽ bàn luận thêm về các ưu nhược điểm cũng như các cách khác mà ta có thể cải tiến được kết quả của mô hình này.  

Mọi người có thể đọc bài báo gốc ở [đây](https://arxiv.org/pdf/1408.5882.pdf).

# Giới thiệu
Để có thể hiểu được bài viết này, sẽ có một vài khái niệm mà mọi người sẽ cần phải biết trước để không bị "ủa là sao ta??!" (biết trước ở đây là mọi người biết được cái công dụng của nó là đủ để hiểu rồi). Các khái niệm đó sẽ bao gồm lớp [embedding](https://towardsdatascience.com/the-secret-to-improved-nlp-an-in-depth-look-at-the-nn-embedding-layer-in-pytorch-6e901e193e16), [convolution](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1), [pooling](https://www.youtube.com/watch?v=8oOgPUO-TBY), [linear](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjTopqI8duDAxX7k1YBHUl3BdYQFnoECAoQAQ&url=https%3A%2F%2Fmedium.com%2Fdatathings%2Flinear-layers-explained-in-a-simple-way-2319a9c2d1aa&usg=AOvVaw1vjNqtJwb0tZp_vA196sc4&opi=89978449). Nhưng không sao, mọi người cứ bấm vào link trong mỗi từ, mình thấy mấy cái trang này giải thích cũng ok.

**!!! Clarify !!!**: Ban đầu là mấy mạng CNN dùng để xử lý ảnh, nhưng mà cũng có vài bài nghiên cứu mang CNN ra khỏi khuôn khổ của xử lí ảnh, họ chuyển nó sang để giải quyết mấy bài toán bên NLP như là semantic matching hay là search query retrieval hay là những bài toán NLP cơ bản khác. Ở đây có một điểm rất thú vị là khi nhắc đến mạng CNN cho chữ thì mọi người nghĩ ngay đến cái bài mà mình vừa link ở trên, nhưng thật ra có một bài viết nền tảng cho cái này mà mình nghĩ mọi người cũng nên quan tâm đó là bài ["A Convolutional Neural Network for Modelling Sentences"](https://arxiv.org/pdf/1404.2188.pdf). Cũng như là một bài khác mà mô mình này lấy cảm hứng đó là bài ["Natural Language Processing (almost) from Scratch"](https://arxiv.org/pdf/1103.0398.pdf). Mình nghĩ hai bài này là nền tảng cho cái bài mà tụi mình phân tích ngày hôm nay

# Cách mạng TextCNN hoạt động
## Biểu diễn chữ dưới dạng vector
Đầu tiên chúng ta cần hiểu word embedding là gì, nếu như các bạn đã hiểu cụ thể nó là gì (hoặc chưa hiểu thì link về embedding ở trên là một nguồn đọc rất tốt) thì tóm gọn lại là việc chúng ta biểu diễn một chữ dưới dạng vector. Như hình vẽ sau: 

![word embedding](/assets/img/blog1/word_embedding.jpg)

Như trong hình vẽ trên thì một chữ của chúng ta sẽ được biểu diễn thành một vector có 7 phần tử (số chiều là 7). Thật ra thì số chiều này là bao nhiêu cũng được, đây là một siêu tham số mà mọi người có thể chọn, với cách nghĩ thông thường thì với một số chiều càng lớn, các từ của chúng ta được biểu diễn bởi một vector dài hơn, do đó mà nắm bắt được nhiều ngữ nghĩa của từ hơn, và cũng như vậy, một số chiều dài hơn tương đương với chi phí tính toán cao hơn. Và cũng trong hình vẽ này, nếu như các chữ được biểu diễn tốt (biểu diễn tốt ở đây là các chữ cùng liên quan tới một nghĩa hay có cái sự liên quan mà mọi người đọc qua thấy cũng hợp lí) thì sẽ có khoảng cách ngắn, hay hiểu là chúng nằm gần nhau cũng được, giống như 'puppy' với 'dog' cùng có nghĩa là con chó, kiểu kiểu vậy 🥲. Ok vậy câu hỏi tiếp theo là giá trị của mấy phần tử này người ta lấy ở đâu? Thì câu trả lời là người ta sẽ random cho mọi người nha, random trong khoảng từ -1 tới 1 trong trường hợp của hình vẽ. Còn trong bài nghiên cứu mà chúng ta tìm hiểu thì các biểu diễn của từ này được lấy từ một mô hình ngôn ngữ học không giám sát trên tập dữ liệu Google News của [Mikolov et al](https://arxiv.org/pdf/1301.3781.pdf). Câu hỏi tiếp theo là tại sao phải làm vậy? Thì câu trả lời đơn giản là 100 tỷ từ được huấn luyện trên tập dữ liệu Google News đã là các chữ được biểu diễn tốt như mình đề cập ở trên, và hơn nữa, làm như vậy sẽ tối ưu hơn việc mà chúng ta chỉ tạo ngẫu nhiên giá trị cho từng phần tử của trong vector của mỗi từ (hiểu nôm na là ngẫu nhiên thôi thì mấy cái từ tự nhiên có ý nghĩa gì đâu 🤨).
## Về mô hình
Cấu trúc mô hình này khá đơn giản, chúng ta sẽ xem tổng quan mô hình ra sao rồi sau đó đào sâu hơn về mặt toán học cũng như cách mà mô hình này hoạt động, các cách biểu diễn đầu ra, đầu vào là như thế nào,v.v.... Dưới đây là cấu trúc mô hình:

![model architecture](/assets/img/blog1/model_structure.png)

Hình trên được lấy từ bài paper tụi mình sẽ nghiên cứu, nên là, tin chuẩn nhế mọi người 🍻

ok đi ăn cơm
# Ứng dụng
Như vậy là giờ mọi người đã hiểu cách mà mô hình này hoạt động. Vậy giờ chúng ta sẽ đưa nó vào thực tiễn để xem đối với các bài toán NLP (cụ thể là bài toán phân tích cảm xúc) thì mô hình của chúng ta hoạt động ra sao. Mọi người có thể bấm vào [đây](https://colab.research.google.com/drive/1lDoW_WrXkMpo9ifYAxKwStkky9lVdTct?usp=sharing) để truy cập vào code của ứng dụng này.

**!!clarify!!**: Phần ứng dụng này sẽ không sử dụng các vector chữ được lấy từ mô hình ngôn ngữ học không giám sát như những gì mà chúng ta bàn luận ở trên, lần này chúng ta sẽ thật sự lấy ngẫu nhiên các giá trị trong mỗi phần tử của vector rồi các giá trị này sẽ được điều chỉnh thông qua quá trình huấn luyện... Nên là, đúng vậy, làm lại từ đầu tới đuôi luôn 💩

Về bộ dữ liệu, thì mình sử dụng một phần của bộ dữ liệu [Amazon Reviews for Sentiment Analysis](https://www.kaggle.com/datasets/bittlingmayer/amazonreviews). Cụ thể hơn thì mình chỉ lấy 40.000 bản ghi cho tập huấn luyện và đâu đó khoảng 1000 bản ghi cho tập kiểm tra. Lưu ý ở đây là mình chỉ lấy mẫu thôi nha, chứ nếu mọi người tải bộ đó về mà chạy trên máy local mà yếu yếu hoặc sử dụng google colab bản bình thường thì nội việc lưu data trong bộ nhớ cũng rất tốn kém á. Bên cạnh đó thì bộ dữ liệu này đã được tiền xử lí từ trước rồi mình mới đẩy lên drive (các bước xử lí như loại bỏ nhãn xấu, v.v... cũng có nhiều hướng tiếp cận mới mẻ mà có dịp thì mình sẽ chia sẻ thêm ). Và để cho đơn giản hơn thì bộ dữ liệu này đang là bộ dữ liệu cân bằng (tỷ lệ nhãn của 2 lớp là bằng nhau trên cả tập huấn luyện và tập kiểm thử). Như 2 hình dưới đây:

![Train distribution](/assets/img/blog1/eda_1.png)

![Text distribution](/assets/img/blog1/eda_2.png)

Về mô hình thì mình tham khảo trong [repository này](https://github.com/gaopinghai/textCNN_pytorch). Repository này được cấu trúc dễ đọc, dễ nắm bắt, nếu có thời gian thì mọi người nên clone về máy xong chạy local cũng ok, nói chung là check it out and give the author a star !!!. Dưới đây là toàn bộ mô hình:
```python
class textCNN(nn.Module):
    def __init__(self, param):
        super(textCNN, self).__init__()
        ci = 1  # input chanel size
        kernel_num = param['kernel_num'] # output chanel size
        kernel_size = param['kernel_size']
        vocab_size = param['vocab_size']
        embed_dim = param['embed_dim']
        dropout = param['dropout']
        class_num = param['class_num']
        self.param = param
        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=1)
        self.conv11 = nn.Conv2d(ci, kernel_num, (kernel_size[0], embed_dim))
        self.conv12 = nn.Conv2d(ci, kernel_num, (kernel_size[1], embed_dim))
        self.conv13 = nn.Conv2d(ci, kernel_num, (kernel_size[2], embed_dim))
        self.dropout = nn.Dropout(dropout)
        self.fc1 = nn.Linear(len(kernel_size) * kernel_num, class_num)

    def init_embed(self, embed_matrix):
        self.embed.weight = nn.Parameter(torch.Tensor(embed_matrix))

    @staticmethod
    def conv_and_pool(x, conv):
        # x: (batch, 1, sentence_length,  )
        x = conv(x)
        # x: (batch, kernel_num, H_out, 1)
        x = F.relu(x.squeeze(3))
        # x: (batch, kernel_num, H_out)
        x = F.max_pool1d(x, x.size(2)).squeeze(2)
        #  (batch, kernel_num)
        return x

    def forward(self, x):
        # x: (batch, sentence_length)
        x = self.embed(x)
        # x: (batch, sentence_length, embed_dim)
        # TODO init embed matrix with pre-trained
        x = x.unsqueeze(1)
        # x: (batch, 1, sentence_length, embed_dim)
        x1 = self.conv_and_pool(x, self.conv11)  # (batch, kernel_num)
        x2 = self.conv_and_pool(x, self.conv12)  # (batch, kernel_num)
        x3 = self.conv_and_pool(x, self.conv13)  # (batch, kernel_num)
        x = torch.cat((x1, x2, x3), 1)  # (batch, 3 * kernel_num)
        x = self.dropout(x)
        logit = F.log_softmax(self.fc1(x), dim=1)
        return logit

    def init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0, 0.01)
                m.bias.data.zero_()
```
Còn dưới đây là các tham số mà mọi người có thể tinh chỉnh để mô hình đạt kết quả tốt hơn:
```python
textCNN_param = {
    'vocab_size': 10000,
    'embed_dim': 256,
    'class_num': 2,
    "kernel_num": 16,
    "kernel_size": [3, 4, 5],
    "dropout": 0.5,
}

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = textCNN(textCNN_param)
model.to(device)

criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.00005)

num_epochs = 50
save_model = './model'
os.makedirs(save_model, exist_ok = True)
model_name = 'model_TextCNN'
```
Và sau khi chạy mô hình với các tham số ở trên (sử dụng GPU T4 free của google colab 😁✌️) thì có được kết quả như hình dưới đây (mô hình này chạy cũng nhanh lắm á mọi người, 50 epochs train vèo 1 phát cỡ 5-10 phút là xong rồi).

![Model Result](/assets/img/blog1/model_result.png)

Một điểm đáng lưu ý là các siêu tham số mình chọn là mình chọn ngẫu nhiên thôi, nhưng kết quả của mô hình thì vẫn rất tốt. Tuy nhiên từ epoch thứ 30 trở đi là cái đường màu cam bắt đầu đi ngang rồi (nói dễ hiểu là mô hình chỉ làm được tới đó thôi), nên để tránh lãng phí tài nguyên tính toán thì mọi người nên thử thêm các phương pháp như [early stopping](https://machinelearningcoban.com/2017/03/04/overfitting/) để tối ưu chi phí huấn luyện mô hình. 


# Ưu và nhược điểm
Talk about the good and not-so-good sides of TextCNN. When does it shine, and where might it struggle a bit? Be honest about what you found.

# Thảo luận thêm
Think big picture. What do your results mean? How does TextCNN compare to other ways of understanding text? Explore where this could be helpful in real-life situations.

# Các thí nghiệm khác
Disclaimer: This part is an extent. Explain how you set things up for your tests. What data did you use, how did you get it ready, and what choices did you make? This helps others understand and try it themselves.

# References

1. Author Name, "Title of Paper 1," *Journal Name*, Year.
   [Link to Paper 1]

2. Author Name, "Title of Paper 2," *Journal Name*, Year.
   [Link to Paper 2]

3. Author Name, "Title of Paper 3," *Journal Name*, Year.
   [Link to Paper 3]