---
title: 'Paper Explained 1: Convolutional Neural Network for Sentence Classification'
date: 2024-01-14
permalink: /posts/2024/01/blog-post-1/
tags:
  - Paper explained
  - Deep Learning
---

Ch√†o m·ªçi ng∆∞·ªùi, ·ªü trong s·ªë ƒë·∫ßu ti√™n, ch√∫ng ta s·∫Ω t√¨m hi·ªÉu v·ªÅ c√°ch th·ª©c m√† m·ªôt m·∫°ng CNN (v·ªën r·∫•t n·ªïi ti·∫øng trong c√°c b√†i to√°n x·ª≠ l√≠ ·∫£nh) l·∫°i c√≥ th·ªÉ √°p d·ª•ng ƒë∆∞·ª£c v√†o trong b√†i to√°n x·ª≠ l√≠ ng√¥n ng·ªØ t·ª± nhi√™n, m√† ·ªü ƒë√¢y c·ª• th·ªÉ l√† b√†i to√°n ph√¢n lo·∫°i c√¢u m√† v·∫´n cho ra m·ªôt k·∫øt qu·∫£ t·ªët. V√† c≈©ng nh∆∞ trong b√†i n√†y, ch√∫ng ta s·∫Ω b√†n lu·∫≠n th√™m v·ªÅ c√°c ∆∞u nh∆∞·ª£c ƒëi·ªÉm c≈©ng nh∆∞ c√°c c√°ch kh√°c m√† ta c√≥ th·ªÉ c·∫£i ti·∫øn ƒë∆∞·ª£c k·∫øt qu·∫£ c·ªßa m√¥ h√¨nh n√†y.  

M·ªçi ng∆∞·ªùi c√≥ th·ªÉ ƒë·ªçc b√†i b√°o g·ªëc ·ªü [ƒë√¢y](https://arxiv.org/pdf/1408.5882.pdf).

# Gi·ªõi thi·ªáu
ƒê·ªÉ c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c b√†i vi·∫øt n√†y, s·∫Ω c√≥ m·ªôt v√†i kh√°i ni·ªám m√† m·ªçi ng∆∞·ªùi s·∫Ω c·∫ßn ph·∫£i bi·∫øt tr∆∞·ªõc ƒë·ªÉ kh√¥ng b·ªã "·ªßa l√† sao ta??!" (bi·∫øt tr∆∞·ªõc ·ªü ƒë√¢y l√† m·ªçi ng∆∞·ªùi bi·∫øt ƒë∆∞·ª£c c√°i c√¥ng d·ª•ng c·ªßa n√≥ l√† ƒë·ªß ƒë·ªÉ hi·ªÉu r·ªìi). C√°c kh√°i ni·ªám ƒë√≥ s·∫Ω bao g·ªìm l·ªõp [embedding](https://towardsdatascience.com/the-secret-to-improved-nlp-an-in-depth-look-at-the-nn-embedding-layer-in-pytorch-6e901e193e16), [convolution](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1), [pooling](https://www.youtube.com/watch?v=8oOgPUO-TBY), [linear](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjTopqI8duDAxX7k1YBHUl3BdYQFnoECAoQAQ&url=https%3A%2F%2Fmedium.com%2Fdatathings%2Flinear-layers-explained-in-a-simple-way-2319a9c2d1aa&usg=AOvVaw1vjNqtJwb0tZp_vA196sc4&opi=89978449). Nh∆∞ng kh√¥ng sao, m·ªçi ng∆∞·ªùi c·ª© b·∫•m v√†o link trong m·ªói t·ª´, m√¨nh th·∫•y m·∫•y c√°i trang n√†y gi·∫£i th√≠ch c≈©ng ok.

**!!! Clarify !!!**: Ban ƒë·∫ßu l√† m·∫•y m·∫°ng CNN d√πng ƒë·ªÉ x·ª≠ l√Ω ·∫£nh, nh∆∞ng m√† c≈©ng c√≥ v√†i b√†i nghi√™n c·ª©u mang CNN ra kh·ªèi khu√¥n kh·ªï c·ªßa x·ª≠ l√≠ ·∫£nh, h·ªç chuy·ªÉn n√≥ sang ƒë·ªÉ gi·∫£i quy·∫øt m·∫•y b√†i to√°n b√™n NLP nh∆∞ l√† semantic matching hay l√† search query retrieval hay l√† nh·ªØng b√†i to√°n NLP c∆° b·∫£n kh√°c. ·ªû ƒë√¢y c√≥ m·ªôt ƒëi·ªÉm r·∫•t th√∫ v·ªã l√† khi nh·∫Øc ƒë·∫øn m·∫°ng CNN cho ch·ªØ th√¨ m·ªçi ng∆∞·ªùi nghƒ© ngay ƒë·∫øn c√°i b√†i m√† m√¨nh v·ª´a link ·ªü tr√™n, nh∆∞ng th·∫≠t ra c√≥ m·ªôt b√†i vi·∫øt n·ªÅn t·∫£ng cho c√°i n√†y m√† m√¨nh nghƒ© m·ªçi ng∆∞·ªùi c≈©ng n√™n quan t√¢m ƒë√≥ l√† b√†i ["A Convolutional Neural Network for Modelling Sentences"](https://arxiv.org/pdf/1404.2188.pdf). M√¨nh nghƒ© b√†i n√†y l√† n·ªÅn t·∫£ng cho c√°i b√†i m√† t·ª•i m√¨nh ph√¢n t√≠ch ng√†y h√¥m nay

# C√°ch m·∫°ng TextCNN ho·∫°t ƒë·ªông
Break down how TextCNN does its job. Talk about its main parts like the layers that process text and how it figures out what the text is saying.

# ·ª®ng d·ª•ng
Nh∆∞ v·∫≠y l√† gi·ªù m·ªçi ng∆∞·ªùi ƒë√£ hi·ªÉu c√°ch m√† m√¥ h√¨nh n√†y ho·∫°t ƒë·ªông. V·∫≠y gi·ªù ch√∫ng ta s·∫Ω ƒë∆∞a n√≥ v√†o th·ª±c ti·ªÖn ƒë·ªÉ xem ƒë·ªëi v·ªõi c√°c b√†i to√°n NLP (c·ª• th·ªÉ l√† b√†i to√°n ph√¢n t√≠ch c·∫£m x√∫c) th√¨ m√¥ h√¨nh c·ªßa ch√∫ng ta ho·∫°t ƒë·ªông ra sao. M·ªçi ng∆∞·ªùi c√≥ th·ªÉ b·∫•m v√†o [ƒë√¢y](https://colab.research.google.com/drive/1lDoW_WrXkMpo9ifYAxKwStkky9lVdTct?usp=sharing) ƒë·ªÉ truy c·∫≠p v√†o code c·ªßa ·ª©ng d·ª•ng n√†y.

V·ªÅ b·ªô d·ªØ li·ªáu, th√¨ m√¨nh s·ª≠ d·ª•ng m·ªôt ph·∫ßn c·ªßa b·ªô d·ªØ li·ªáu [Amazon Reviews for Sentiment Analysis](https://www.kaggle.com/datasets/bittlingmayer/amazonreviews). C·ª• th·ªÉ h∆°n th√¨ m√¨nh ch·ªâ l·∫•y 40.000 b·∫£n ghi cho t·∫≠p hu·∫•n luy·ªán v√† ƒë√¢u ƒë√≥ kho·∫£ng 1000 b·∫£n ghi cho t·∫≠p ki·ªÉm tra. L∆∞u √Ω ·ªü ƒë√¢y l√† m√¨nh ch·ªâ l·∫•y m·∫´u th√¥i nha, ch·ª© n·∫øu m·ªçi ng∆∞·ªùi t·∫£i b·ªô ƒë√≥ v·ªÅ m√† ch·∫°y tr√™n m√°y local m√† y·∫øu y·∫øu ho·∫∑c s·ª≠ d·ª•ng google colab b·∫£n b√¨nh th∆∞·ªùng th√¨ n·ªôi vi·ªác l∆∞u data trong b·ªô nh·ªõ c≈©ng r·∫•t t·ªën k√©m √°. B√™n c·∫°nh ƒë√≥ th√¨ b·ªô d·ªØ li·ªáu n√†y ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√≠ t·ª´ tr∆∞·ªõc r·ªìi m√¨nh m·ªõi ƒë·∫©y l√™n drive (c√°c b∆∞·ªõc x·ª≠ l√≠ nh∆∞ lo·∫°i b·ªè nh√£n x·∫•u, v.v... c≈©ng c√≥ nhi·ªÅu h∆∞·ªõng ti·∫øp c·∫≠n m·ªõi m·∫ª m√† c√≥ d·ªãp th√¨ m√¨nh s·∫Ω chia s·∫ª th√™m ). V√† ƒë·ªÉ cho ƒë∆°n gi·∫£n h∆°n th√¨ b·ªô d·ªØ li·ªáu n√†y ƒëang l√† b·ªô d·ªØ li·ªáu c√¢n b·∫±ng (t·ª∑ l·ªá nh√£n c·ªßa 2 l·ªõp l√† b·∫±ng nhau tr√™n c·∫£ t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm th·ª≠). Nh∆∞ 2 h√¨nh d∆∞·ªõi ƒë√¢y:

![Train distribution](/assets/img/blog1/eda_1.png)

![Text distribution](/assets/img/blog1/eda_2.png)

V·ªÅ m√¥ h√¨nh th√¨ m√¨nh tham kh·∫£o trong [repository n√†y](https://github.com/gaopinghai/textCNN_pytorch). Repository n√†y ƒë∆∞·ª£c c·∫•u tr√∫c d·ªÖ ƒë·ªçc, d·ªÖ n·∫Øm b·∫Øt, n·∫øu c√≥ th·ªùi gian th√¨ m·ªçi ng∆∞·ªùi n√™n clone v·ªÅ m√°y xong ch·∫°y local c≈©ng ok, n√≥i chung l√† check it out and give the author a star !!!. D∆∞·ªõi ƒë√¢y l√† to√†n b·ªô m√¥ h√¨nh:
```python
class textCNN(nn.Module):
    def __init__(self, param):
        super(textCNN, self).__init__()
        ci = 1  # input chanel size
        kernel_num = param['kernel_num'] # output chanel size
        kernel_size = param['kernel_size']
        vocab_size = param['vocab_size']
        embed_dim = param['embed_dim']
        dropout = param['dropout']
        class_num = param['class_num']
        self.param = param
        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=1)
        self.conv11 = nn.Conv2d(ci, kernel_num, (kernel_size[0], embed_dim))
        self.conv12 = nn.Conv2d(ci, kernel_num, (kernel_size[1], embed_dim))
        self.conv13 = nn.Conv2d(ci, kernel_num, (kernel_size[2], embed_dim))
        self.dropout = nn.Dropout(dropout)
        self.fc1 = nn.Linear(len(kernel_size) * kernel_num, class_num)

    def init_embed(self, embed_matrix):
        self.embed.weight = nn.Parameter(torch.Tensor(embed_matrix))

    @staticmethod
    def conv_and_pool(x, conv):
        # x: (batch, 1, sentence_length,  )
        x = conv(x)
        # x: (batch, kernel_num, H_out, 1)
        x = F.relu(x.squeeze(3))
        # x: (batch, kernel_num, H_out)
        x = F.max_pool1d(x, x.size(2)).squeeze(2)
        #  (batch, kernel_num)
        return x

    def forward(self, x):
        # x: (batch, sentence_length)
        x = self.embed(x)
        # x: (batch, sentence_length, embed_dim)
        # TODO init embed matrix with pre-trained
        x = x.unsqueeze(1)
        # x: (batch, 1, sentence_length, embed_dim)
        x1 = self.conv_and_pool(x, self.conv11)  # (batch, kernel_num)
        x2 = self.conv_and_pool(x, self.conv12)  # (batch, kernel_num)
        x3 = self.conv_and_pool(x, self.conv13)  # (batch, kernel_num)
        x = torch.cat((x1, x2, x3), 1)  # (batch, 3 * kernel_num)
        x = self.dropout(x)
        logit = F.log_softmax(self.fc1(x), dim=1)
        return logit

    def init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0, 0.01)
                m.bias.data.zero_()
```
C√≤n d∆∞·ªõi ƒë√¢y l√† c√°c tham s·ªë m√† m·ªçi ng∆∞·ªùi c√≥ th·ªÉ tinh ch·ªânh ƒë·ªÉ m√¥ h√¨nh ƒë·∫°t k·∫øt qu·∫£ t·ªët h∆°n:
```python
textCNN_param = {
    'vocab_size': 10000,
    'embed_dim': 256,
    'class_num': 2,
    "kernel_num": 16,
    "kernel_size": [3, 4, 5],
    "dropout": 0.5,
}

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = textCNN(textCNN_param)
model.to(device)

criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.00005)

num_epochs = 50
save_model = './model'
os.makedirs(save_model, exist_ok = True)
model_name = 'model_TextCNN'
```
V√† sau khi ch·∫°y m√¥ h√¨nh v·ªõi c√°c tham s·ªë ·ªü tr√™n (s·ª≠ d·ª•ng GPU T4 free c·ªßa google colab üòÅ‚úåÔ∏è) th√¨ c√≥ ƒë∆∞·ª£c k·∫øt qu·∫£ nh∆∞ h√¨nh d∆∞·ªõi ƒë√¢y (m√¥ h√¨nh n√†y ch·∫°y c≈©ng nhanh l·∫Øm √° m·ªçi ng∆∞·ªùi, 50 epochs train v√®o 1 ph√°t c·ª° 5-10 ph√∫t l√† xong r·ªìi).

![Model Result](/assets/img/blog1/model_result.png)


# ∆Øu v√† nh∆∞·ª£c ƒëi·ªÉm
Talk about the good and not-so-good sides of TextCNN. When does it shine, and where might it struggle a bit? Be honest about what you found.

# Th·∫£o lu·∫≠n th√™m
Think big picture. What do your results mean? How does TextCNN compare to other ways of understanding text? Explore where this could be helpful in real-life situations.

# C√°c th√≠ nghi·ªám kh√°c
Disclaimer: This part is an extent. Explain how you set things up for your tests. What data did you use, how did you get it ready, and what choices did you make? This helps others understand and try it themselves.

# References

1. Author Name, "Title of Paper 1," *Journal Name*, Year.
   [Link to Paper 1]

2. Author Name, "Title of Paper 2," *Journal Name*, Year.
   [Link to Paper 2]

3. Author Name, "Title of Paper 3," *Journal Name*, Year.
   [Link to Paper 3]