---
title: 'Paper Explained 2: Pay Attention to MLPs'
date: 2024-02-02
permalink: /posts/2024/02/blog-post-2/
tags:
  - Paper explained
  - Deep Learning
published: true
---

📣 Attention Attention Attention, attention này attention kia, quá nhiều attention. 😅 Trong bài viết này chúng ta sẽ cùng thảo luận về một cách khác cũng đáng nhận được attention, mặc dù không phải attention. 🤔 Chủ đề hôm này của chúng ta sẽ bàn về MLP (cụ thể hơn thì là một biến thể của mạng MLP truyền thống) nói chung và gMLP nói riêng (Highlight của biến thể này là Spatial Gating Unit - Một đơn vị để kiểm soát thông tin). 🎯 Bài này khá hay, cùng đọc nhế !!! 📚🎉

# Giới thiệu.
**!! NOTICE !!** Chữ kiến trúc với chữ mô hình nó hơi nhạy cảm mọi người, nên trong bài này, mình nói kiến trúc là nói chung, còn mô hình là nói cụ thể, chỉ đích danh mô hình đó luôn. z thui, cheers

Các bạn mà có hay cập nhật các thông tin về mấy cái mô hình ngôn ngữ lớn (LLM) như mấy cái Mistral, LLaMa, v.v... thì chắc các bạn cũng đã biết cái phần cốt lõi của mấy mô hình khủng này là kiến trúc Transformer (mà cụ thể hơn là cơ chế attention của nó). 

Những cái kiến trúc của Transformer bao gồm 2 thành phần chính là đó là một cái **kiến trúc recurrent-free** (do đó mà nó cho phép tính cái representation của các tokens một cách song song) và khối **multi-head self-attention** cho phép đa dạng và tổng hợp thông tin giữa các token với nhau. 

Vậy câu hỏi đặt ra ở đây là: "**Cái khối attention đó có cần thiết không?**". Bởi vì cái cơ chế attention giới thiệu một cái gọi là "Thiên kiến quy nạp" (Inductive bias) (Cái inductive bias hiểu theo nghĩa khác là khả năng generalize của mô hình tới đâu đối với các giả thuyết khác nhau) và cái inductive bias này sẽ tốt khi mà các **sự tương tác giữa các tokens trong câu nên được tham số hóa một cách dynamic dựa trên biểu diễn đầu vào** (phần dưới mình có giải thích thêm về inductive bias), tuy nhiên khi nhìn theo một cái góc nhìn khác thì những **cấu trúc MLP với các tham số tĩnh cho phép nó biểu diễn một hàm bất kỳ nào đó** (theo như Universal Approximation Theorem - đại khái cái theorem này cho rằng các FFN (Feed-forward network) có thể xấp xỉ bất cứ hàm liên tục nào bằng).   

Để trả lời câu hỏi ở trên, tác giả mới giới thiệu một mô hình mới gọi là gMLP bởi vì nó được tạo ra từ kiến trúc MLP và một đơn vị cổng (gating unit). 

Và spoil trước, nó hiệu quả.

Phần này sẽ dùng để giải thích các khái niệm liên quan mà nó ít phổ biến hơn. Nếu ở trên mọi người đã hiểu rồi thì cứ mạnh dạng xuốgn phần mô hình hoạt động luôn cũng được, còn nếu mà những gì mình nói ở trên chưa cụ thể thì phần này sẽ giải thích kĩ hơn để mọi người có thể hiểu về cách mô hình hoạt động. Có một bài giải thích rất hay và rõ ràng, mình sẽ để trong [link](https://towardsdatascience.com/a-fairy-tale-of-the-inductive-bias-d418fc61726c) này nha.

# Inductive bias (Thiên kiến quy nạp)
Để dễ hiểu hơn thì mọi người tưởng tượng: Trước tới giờ mọi người chỉ thấy một đàn thiên nga đang bơi trong một cái hồ gần nhà thôi, và đây là nơi duy nhất trong đời mà mọi người có thể quan sát mấy con thiên nga này. Một đống giả thuyết về mấy con thiên nga này mà các bạn có thể đặt ra như sau: "Thiên nga là loài có màu trắng", "Thiên nga chỉ biết bơi", "Thiên nga không biết bay", "Thiên nga đen không tồn tại",v.v... Nói chung là mọi người có đưa ra giả thuyết nào cũng được, do đó mà có vô số giả thuyết có thể được đưa ra, đúng hay sai là chuyện khác. 

![swan](/assets/img/blog2/whitevsblack.png)

Theo lý thuyết, không gian giả thuyết là vô tận (tức là mọi người nghĩ ra bao nhiêu cái giả thuyết cũng được, không giới hạn, đúng sai bàn sau). Cái inductive bias có thể được hiểu như là các giả thuyết được ưu tiên hơn trong không gian giả thuyết. Lấy ví dụ như mấy bài toán như hồi quy tuyến tính, mọi người đang giả định tồn tại mối quan hệ tuyến tính giữa các điểm dữ liệu, và bằng cái giả định này, mọi người đồng thời giới hạn không gian giả thuyết xuống còn mối quan hệ tuyến tính. 

![inductive_bias](/assets/img/blog2/inductive_bias.png)

Vậy trong khuôn khổ của machine learning thì điều này là sao? Mọi người đang có đa dạng dữ liệu (ảnh, chữ, tín hiệu, v.v...) và vô vàn loại mô hình khác nhau (tích chập, hồi quy, v.v...) và mỗi mô hình khác nhau này đều có một cái inductive bias để nó hoạt động tốt khác nhau. Ví dụ như là các mạng CNN được xây dựng dựa trên giả thuyết các điểm pixel nằm gần nhau thì có liên quan tới nhau và mô hình nên học được cái sự liên quan này. 

Và nhân vật chính của chúng ta là kiến trúc Transformer, mô hình này không có một cái inductive bias mạnh, do đó cho phép mô hình khái quát hóa tốt hơn khi nó được huấn luyện với nhiều dữ liệu hơn. Lí do đơn giản bởi vì **kiến trúc Transformer không đặt ra các giả định về đầu vào của mô hình**, mà nó sẽ học thông qua cơ chế attention để biết các đầu vào khác nhau ở các vị trí khác nhau tương tác như thế nào.

# The Universal Approximation Theorem
Nói theo ý nghĩa toán thì các cấu trúc mạng neural nào thật chất cũng là đi tìm một hàm số $f$ sao cho $y = f(x)$ với $x$ là dữ liệu đầu vào còn $y$ là dữ liệu đầu ra. Vậy định lý này muốn nói rằng các mạng Neural cũng có tính chất universality, tức là bất kể hàm $f(x)$ ra sao, luôn có một mạng có thể xấp xỉ kết quả đó. 




# Cách mô hình hoạt động

# Ứng dụng

# Phân tích ưu và nhược điểm

# Kết luận

# Thảo luận thêm