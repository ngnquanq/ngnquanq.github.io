---
title: 'Paper Explained 2: Pay Attention to MLPs'
date: 2024-02-02
permalink: /posts/2024/02/blog-post-2/
tags:
  - Paper explained
  - Deep Learning
published: true
---

ğŸ“£ Attention Attention Attention, attention nÃ y attention kia, quÃ¡ nhiá»u attention. ğŸ˜… Trong bÃ i viáº¿t nÃ y chÃºng ta sáº½ cÃ¹ng tháº£o luáº­n vá» má»™t cÃ¡ch khÃ¡c cÅ©ng Ä‘Ã¡ng nháº­n Ä‘Æ°á»£c attention, máº·c dÃ¹ khÃ´ng pháº£i attention. ğŸ¤” Chá»§ Ä‘á» hÃ´m nÃ y cá»§a chÃºng ta sáº½ bÃ n vá» MLP (cá»¥ thá»ƒ hÆ¡n thÃ¬ lÃ  má»™t biáº¿n thá»ƒ cá»§a máº¡ng MLP truyá»n thá»‘ng) nÃ³i chung vÃ  gMLP nÃ³i riÃªng (Highlight cá»§a biáº¿n thá»ƒ nÃ y lÃ  Spatial Gating Unit - Má»™t Ä‘Æ¡n vá»‹ Ä‘á»ƒ kiá»ƒm soÃ¡t thÃ´ng tin). ğŸ¯ BÃ i nÃ y khÃ¡ hay, cÃ¹ng Ä‘á»c nháº¿ !!! ğŸ“šğŸ‰

# Giá»›i thiá»‡u

# CÃ¡ch mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng

# á»¨ng dá»¥ng

# PhÃ¢n tÃ­ch Æ°u vÃ  nhÆ°á»£c Ä‘iá»ƒm

# Káº¿t luáº­n

# Tháº£o luáº­n thÃªm