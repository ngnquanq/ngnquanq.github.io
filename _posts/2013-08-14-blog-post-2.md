---
title: 'Paper Explained 2: Pay Attention to MLPs'
date: 2024-02-02
permalink: /posts/2024/02/blog-post-2/
tags:
  - Paper explained
  - Deep Learning
published: true
---

📣 Attention Attention Attention, attention này attention kia, quá nhiều attention. 😅 Trong bài viết này chúng ta sẽ cùng thảo luận về một cách khác cũng đáng nhận được attention, mặc dù không phải attention. 🤔 Chủ đề hôm này của chúng ta sẽ bàn về MLP (cụ thể hơn thì là một biến thể của mạng MLP truyền thống) nói chung và gMLP nói riêng (Highlight của biến thể này là Spatial Gating Unit - Một đơn vị để kiểm soát thông tin). 🎯 Bài này khá hay, cùng đọc nhế !!! 📚🎉

# Giới thiệu.
**!! NOTICE !!** Chữ kiến trúc với chữ mô hình nó hơi nhạy cảm mọi người, nên trong bài này, mình nói kiến trúc là nói chung, còn mô hình là nói cụ thể, chỉ đích danh mô hình đó luôn. z thui, cheers

Các bạn mà có hay cập nhật các thông tin về mấy cái mô hình ngôn ngữ lớn (LLM) như mấy cái Mistral, LLaMa, v.v... thì chắc các bạn cũng đã biết cái phần cốt lõi của mấy mô hình khủng này là kiến trúc Transformer (mà cụ thể hơn là cơ chế attention của nó). 

Những cái kiến trúc của Transformer bao gồm 2 thành phần chính là đó là một cái **kiến trúc recurrent-free** (do đó mà nó cho phép tính cái representation của các tokens một cách song song) và khối **multi-head self-attention** cho phép đa dạng và tổng hợp thông tin giữa các token với nhau. 

Vậy câu hỏi đặt ra ở đây là: "**Cái khối attention đó có cần thiết không?**". Bởi vì cái cơ chế attention giới thiệu một cái gọi là "Thiên kiến quy nạp" (Inductive bias) (Cái inductive bias này kiểu như mô hình làm tốt ra sao với những gì mà nó chưa thấy - Hiểu theo nghĩa khác là khả năng generalize của mô hình tới đâu) và cái inductive bias này sẽ tốt khi mà các **sự tương tác giữa các tokens trong câu nên được tham số hóa một cách dynamic dựa trên biểu diễn đầu vào**, tuy nhiên khi nhìn theo một cái góc nhìn khác thì những **cấu trúc MLP với các tham số tĩnh cho phép nó biểu diễn một hàm bất kỳ nào đó**.   

Để trả lời câu hỏi ở trên, tác giả mới giới thiệu một mô hình mới gọi là gMLP bởi vì nó được tạo ra từ kiến trúc MLP và một đơn vị cổng (gating unit). 

Và spoil trước, nó hiệu quả.

# Cách mô hình hoạt động

# Ứng dụng

# Phân tích ưu và nhược điểm

# Kết luận

# Thảo luận thêm