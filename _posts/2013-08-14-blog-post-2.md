---
title: 'Paper Explained 2: Pay Attention to MLPs'
date: 2024-02-02
permalink: /posts/2024/02/blog-post-2/
tags:
  - Paper explained
  - Deep Learning
published: true
---

ğŸ“£ Attention Attention Attention, attention nÃ y attention kia, quÃ¡ nhiá»u attention. ğŸ˜… Trong bÃ i viáº¿t nÃ y chÃºng ta sáº½ cÃ¹ng tháº£o luáº­n vá» má»™t cÃ¡ch khÃ¡c cÅ©ng Ä‘Ã¡ng nháº­n Ä‘Æ°á»£c attention, máº·c dÃ¹ khÃ´ng pháº£i attention. ğŸ¤” Chá»§ Ä‘á» hÃ´m nÃ y cá»§a chÃºng ta sáº½ bÃ n vá» MLP (cá»¥ thá»ƒ hÆ¡n thÃ¬ lÃ  má»™t biáº¿n thá»ƒ cá»§a máº¡ng MLP truyá»n thá»‘ng) nÃ³i chung vÃ  gMLP nÃ³i riÃªng (Highlight cá»§a biáº¿n thá»ƒ nÃ y lÃ  Spatial Gating Unit - Má»™t Ä‘Æ¡n vá»‹ Ä‘á»ƒ kiá»ƒm soÃ¡t thÃ´ng tin). ğŸ¯ BÃ i nÃ y khÃ¡ hay, cÃ¹ng Ä‘á»c nháº¿ !!! ğŸ“šğŸ‰

# Giá»›i thiá»‡u.
**!! NOTICE !!** Chá»¯ kiáº¿n trÃºc vá»›i chá»¯ mÃ´ hÃ¬nh nÃ³ hÆ¡i nháº¡y cáº£m má»i ngÆ°á»i, nÃªn trong bÃ i nÃ y, mÃ¬nh nÃ³i kiáº¿n trÃºc lÃ  nÃ³i chung, cÃ²n mÃ´ hÃ¬nh lÃ  nÃ³i cá»¥ thá»ƒ, chá»‰ Ä‘Ã­ch danh mÃ´ hÃ¬nh Ä‘Ã³ luÃ´n. z thui, cheers

CÃ¡c báº¡n mÃ  cÃ³ hay cáº­p nháº­t cÃ¡c thÃ´ng tin vá» máº¥y cÃ¡i mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) nhÆ° máº¥y cÃ¡i Mistral, LLaMa, v.v... thÃ¬ cháº¯c cÃ¡c báº¡n cÅ©ng Ä‘Ã£ biáº¿t cÃ¡i pháº§n cá»‘t lÃµi cá»§a máº¥y mÃ´ hÃ¬nh khá»§ng nÃ y lÃ  kiáº¿n trÃºc Transformer (mÃ  cá»¥ thá»ƒ hÆ¡n lÃ  cÆ¡ cháº¿ attention cá»§a nÃ³). 

Nhá»¯ng cÃ¡i kiáº¿n trÃºc cá»§a Transformer bao gá»“m 2 thÃ nh pháº§n chÃ­nh lÃ  Ä‘Ã³ lÃ  má»™t cÃ¡i **kiáº¿n trÃºc recurrent-free** (do Ä‘Ã³ mÃ  nÃ³ cho phÃ©p tÃ­nh cÃ¡i representation cá»§a cÃ¡c tokens má»™t cÃ¡ch song song) vÃ  khá»‘i **multi-head self-attention** cho phÃ©p Ä‘a dáº¡ng vÃ  tá»•ng há»£p thÃ´ng tin giá»¯a cÃ¡c token vá»›i nhau. 

Váº­y cÃ¢u há»i Ä‘áº·t ra á»Ÿ Ä‘Ã¢y lÃ : "**CÃ¡i khá»‘i attention Ä‘Ã³ cÃ³ cáº§n thiáº¿t khÃ´ng?**". Bá»Ÿi vÃ¬ cÃ¡i cÆ¡ cháº¿ attention giá»›i thiá»‡u má»™t cÃ¡i gá»i lÃ  "ThiÃªn kiáº¿n quy náº¡p" (Inductive bias) (CÃ¡i inductive bias nÃ y kiá»ƒu nhÆ° mÃ´ hÃ¬nh lÃ m tá»‘t ra sao vá»›i nhá»¯ng gÃ¬ mÃ  nÃ³ chÆ°a tháº¥y - Hiá»ƒu theo nghÄ©a khÃ¡c lÃ  kháº£ nÄƒng generalize cá»§a mÃ´ hÃ¬nh tá»›i Ä‘Ã¢u) vÃ  cÃ¡i inductive bias nÃ y sáº½ tá»‘t khi mÃ  cÃ¡c **sá»± tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c tokens trong cÃ¢u nÃªn Ä‘Æ°á»£c tham sá»‘ hÃ³a má»™t cÃ¡ch dynamic dá»±a trÃªn biá»ƒu diá»…n Ä‘áº§u vÃ o**, tuy nhiÃªn khi nhÃ¬n theo má»™t cÃ¡i gÃ³c nhÃ¬n khÃ¡c thÃ¬ nhá»¯ng **cáº¥u trÃºc MLP vá»›i cÃ¡c tham sá»‘ tÄ©nh cho phÃ©p nÃ³ biá»ƒu diá»…n má»™t hÃ m báº¥t ká»³ nÃ o Ä‘Ã³**.   

Äá»ƒ tráº£ lá»i cÃ¢u há»i á»Ÿ trÃªn, tÃ¡c giáº£ má»›i giá»›i thiá»‡u má»™t mÃ´ hÃ¬nh má»›i gá»i lÃ  gMLP bá»Ÿi vÃ¬ nÃ³ Ä‘Æ°á»£c táº¡o ra tá»« kiáº¿n trÃºc MLP vÃ  má»™t Ä‘Æ¡n vá»‹ cá»•ng (gating unit). 

VÃ  spoil trÆ°á»›c, nÃ³ hiá»‡u quáº£.

# CÃ¡ch mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng

# á»¨ng dá»¥ng

# PhÃ¢n tÃ­ch Æ°u vÃ  nhÆ°á»£c Ä‘iá»ƒm

# Káº¿t luáº­n

# Tháº£o luáº­n thÃªm