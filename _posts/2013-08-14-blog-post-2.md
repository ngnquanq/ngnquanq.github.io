---
title: 'Paper Explained 2: Pay Attention to MLPs'
date: 2024-02-02
permalink: /posts/2024/02/blog-post-2/
tags:
  - Paper explained
  - Deep Learning
published: true
---

ğŸ“£ Attention Attention Attention, attention nÃ y attention kia, quÃ¡ nhiá»u attention. ğŸ˜… Trong bÃ i viáº¿t nÃ y chÃºng ta sáº½ cÃ¹ng tháº£o luáº­n vá» má»™t cÃ¡ch khÃ¡c cÅ©ng Ä‘Ã¡ng nháº­n Ä‘Æ°á»£c attention, máº·c dÃ¹ khÃ´ng pháº£i attention. ğŸ¤” Chá»§ Ä‘á» hÃ´m nÃ y cá»§a chÃºng ta sáº½ bÃ n vá» MLP (cá»¥ thá»ƒ hÆ¡n thÃ¬ lÃ  má»™t biáº¿n thá»ƒ cá»§a máº¡ng MLP truyá»n thá»‘ng) nÃ³i chung vÃ  gMLP nÃ³i riÃªng (Highlight cá»§a biáº¿n thá»ƒ nÃ y lÃ  Spatial Gating Unit - Má»™t Ä‘Æ¡n vá»‹ Ä‘á»ƒ kiá»ƒm soÃ¡t thÃ´ng tin). ğŸ¯ BÃ i nÃ y khÃ¡ hay, cÃ¹ng Ä‘á»c nháº¿ !!! ğŸ“šğŸ‰

# Giá»›i thiá»‡u.
**!! NOTICE !!** Chá»¯ kiáº¿n trÃºc vá»›i chá»¯ mÃ´ hÃ¬nh nÃ³ hÆ¡i nháº¡y cáº£m má»i ngÆ°á»i, nÃªn trong bÃ i nÃ y, mÃ¬nh nÃ³i kiáº¿n trÃºc lÃ  nÃ³i chung, cÃ²n mÃ´ hÃ¬nh lÃ  nÃ³i cá»¥ thá»ƒ, chá»‰ Ä‘Ã­ch danh mÃ´ hÃ¬nh Ä‘Ã³ luÃ´n. z thui, cheers

CÃ¡c báº¡n mÃ  cÃ³ hay cáº­p nháº­t cÃ¡c thÃ´ng tin vá» máº¥y cÃ¡i mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) nhÆ° máº¥y cÃ¡i Mistral, LLaMa, v.v... thÃ¬ cháº¯c cÃ¡c báº¡n cÅ©ng Ä‘Ã£ biáº¿t cÃ¡i pháº§n cá»‘t lÃµi cá»§a máº¥y mÃ´ hÃ¬nh khá»§ng nÃ y lÃ  kiáº¿n trÃºc Transformer (mÃ  cá»¥ thá»ƒ hÆ¡n lÃ  cÆ¡ cháº¿ attention cá»§a nÃ³). 

Nhá»¯ng cÃ¡i kiáº¿n trÃºc cá»§a Transformer bao gá»“m 2 thÃ nh pháº§n chÃ­nh lÃ  Ä‘Ã³ lÃ  má»™t cÃ¡i **kiáº¿n trÃºc recurrent-free** (do Ä‘Ã³ mÃ  nÃ³ cho phÃ©p tÃ­nh cÃ¡i representation cá»§a cÃ¡c tokens má»™t cÃ¡ch song song) vÃ  khá»‘i **multi-head self-attention** cho phÃ©p Ä‘a dáº¡ng vÃ  tá»•ng há»£p thÃ´ng tin giá»¯a cÃ¡c token vá»›i nhau. 

Váº­y cÃ¢u há»i Ä‘áº·t ra á»Ÿ Ä‘Ã¢y lÃ : "**CÃ¡i khá»‘i attention Ä‘Ã³ cÃ³ cáº§n thiáº¿t khÃ´ng?**". Bá»Ÿi vÃ¬ xÃ©t theo má»™t máº·t nÃ o Ä‘Ã³, cÆ¡ cháº¿ attention cÃ³ má»™t cÃ¡i inductive bias Ä‘Ã³ lÃ  **tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c token nÃªn Ä‘Æ°á»£c tham sá»‘ má»™t cÃ¡ch dynamic dá»±a trÃªn biá»ƒu diá»…n Ä‘áº§u vÃ o**, tá»©c lÃ  cÃ¡c cÃ¡i representation cÃ³ thá»ƒ thay Ä‘á»•i dá»±a trÃªn input cá»§a mÃ´ hÃ¬nh. NhÆ°ng xÃ©t theo máº·t khÃ¡c, dá»±a vÃ o Universal Approximation Theorem, báº¥t kÃ¬ cáº¥u trÃºc MLP nÃ o vá»›i cÅ©ng cÃ³ thá»ƒ **xáº¥p xá»‰ má»™t hÃ m nÃ o Ä‘Ã³ vá»›i tham sá»‘ cá»‘ Ä‘á»‹nh**. VÃ  tá»« nhá»¯ng suy nghÄ© trÃªn, mÃ¬nh cÃ³ thá»ƒ chuyá»ƒn cÃ¢u há»i vá»«a Ä‘áº·t ra sang má»™t cÃ¢u khÃ¡c cÅ©ng khÃ¡ tÆ°Æ¡ng tá»±: "**Äiá»u gÃ¬ Ä‘Ã³ng gÃ³p tá»›i thÃ nh cÃ´ng cá»§a mÃ´ hÃ¬nh hÆ¡n? Má»™t cÃ¡i inductive bias yáº¿u trong cÆ¡ cháº¿ attention hay kháº£ nÄƒng xáº¥p xá»‰ báº¥t ká»³ hÃ m toÃ¡n há»c nÃ o cá»§a cÃ¡c máº¡ng Neural?**"

ThÃ¬ bÃ i nghiÃªn cá»©u cá»§a cÃ¡c tÃ¡c giáº£, bÃªn cáº¡nh viá»‡c tráº£ lá»i cÃ¢u há»i á»Ÿ trÃªn, tÃ¡c giáº£ má»›i giá»›i thiá»‡u má»™t mÃ´ hÃ¬nh má»›i gá»i lÃ  gMLP bá»Ÿi vÃ¬ nÃ³ Ä‘Æ°á»£c táº¡o ra tá»« kiáº¿n trÃºc MLP vÃ  má»™t Ä‘Æ¡n vá»‹ cá»•ng (gating unit). 

VÃ  spoil trÆ°á»›c, nÃ³ hiá»‡u quáº£.

# Inductive bias (ThiÃªn kiáº¿n quy náº¡p)
Äá»ƒ dá»… hiá»ƒu hÆ¡n thÃ¬ má»i ngÆ°á»i tÆ°á»Ÿng tÆ°á»£ng: TrÆ°á»›c tá»›i giá» má»i ngÆ°á»i chá»‰ tháº¥y má»™t Ä‘Ã n thiÃªn nga Ä‘ang bÆ¡i trong má»™t cÃ¡i há»“ gáº§n nhÃ  thÃ´i, vÃ  Ä‘Ã¢y lÃ  nÆ¡i duy nháº¥t trong Ä‘á»i mÃ  má»i ngÆ°á»i cÃ³ thá»ƒ quan sÃ¡t máº¥y con thiÃªn nga nÃ y. Má»™t Ä‘á»‘ng giáº£ thuyáº¿t vá» máº¥y con thiÃªn nga nÃ y mÃ  cÃ¡c báº¡n cÃ³ thá»ƒ Ä‘áº·t ra nhÆ° sau: "ThiÃªn nga lÃ  loÃ i cÃ³ mÃ u tráº¯ng", "ThiÃªn nga chá»‰ biáº¿t bÆ¡i", "ThiÃªn nga khÃ´ng biáº¿t bay", "ThiÃªn nga Ä‘en khÃ´ng tá»“n táº¡i",v.v... NÃ³i chung lÃ  má»i ngÆ°á»i cÃ³ Ä‘Æ°a ra giáº£ thuyáº¿t nÃ o cÅ©ng Ä‘Æ°á»£c, do Ä‘Ã³ mÃ  cÃ³ vÃ´ sá»‘ giáº£ thuyáº¿t cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Æ°a ra, Ä‘Ãºng hay sai lÃ  chuyá»‡n khÃ¡c. 

![swan](/assets/img/blog2/whitevsblack.png)

Theo lÃ½ thuyáº¿t, khÃ´ng gian giáº£ thuyáº¿t lÃ  vÃ´ táº­n (tá»©c lÃ  má»i ngÆ°á»i nghÄ© ra bao nhiÃªu cÃ¡i giáº£ thuyáº¿t cÅ©ng Ä‘Æ°á»£c, khÃ´ng giá»›i háº¡n, Ä‘Ãºng sai bÃ n sau). CÃ¡i inductive bias cÃ³ thá»ƒ Ä‘Æ°á»£c hiá»ƒu nhÆ° lÃ  cÃ¡c giáº£ thuyáº¿t Ä‘Æ°á»£c Æ°u tiÃªn hÆ¡n trong khÃ´ng gian giáº£ thuyáº¿t. Láº¥y vÃ­ dá»¥ nhÆ° máº¥y bÃ i toÃ¡n nhÆ° há»“i quy tuyáº¿n tÃ­nh, má»i ngÆ°á»i Ä‘ang giáº£ Ä‘á»‹nh tá»“n táº¡i má»‘i quan há»‡ tuyáº¿n tÃ­nh giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u, vÃ  báº±ng cÃ¡i giáº£ Ä‘á»‹nh nÃ y, má»i ngÆ°á»i Ä‘á»“ng thá»i giá»›i háº¡n khÃ´ng gian giáº£ thuyáº¿t xuá»‘ng cÃ²n má»‘i quan há»‡ tuyáº¿n tÃ­nh. 

![inductive_bias](/assets/img/blog2/inductive_bias.png)

Váº­y trong khuÃ´n khá»• cá»§a machine learning thÃ¬ Ä‘iá»u nÃ y lÃ  sao? Má»i ngÆ°á»i Ä‘ang cÃ³ Ä‘a dáº¡ng dá»¯ liá»‡u (áº£nh, chá»¯, tÃ­n hiá»‡u, v.v...) vÃ  vÃ´ vÃ n loáº¡i mÃ´ hÃ¬nh khÃ¡c nhau (tÃ­ch cháº­p, há»“i quy, v.v...) vÃ  má»—i mÃ´ hÃ¬nh khÃ¡c nhau nÃ y Ä‘á»u cÃ³ má»™t cÃ¡i inductive bias Ä‘á»ƒ nÃ³ hoáº¡t Ä‘á»™ng tá»‘t khÃ¡c nhau. VÃ­ dá»¥ nhÆ° lÃ  cÃ¡c máº¡ng CNN Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn giáº£ thuyáº¿t cÃ¡c Ä‘iá»ƒm pixel náº±m gáº§n nhau thÃ¬ cÃ³ liÃªn quan tá»›i nhau vÃ  mÃ´ hÃ¬nh nÃªn há»c Ä‘Æ°á»£c cÃ¡i sá»± liÃªn quan nÃ y. 

VÃ  nhÃ¢n váº­t chÃ­nh cá»§a chÃºng ta lÃ  kiáº¿n trÃºc Transformer, mÃ´ hÃ¬nh nÃ y khÃ´ng cÃ³ má»™t cÃ¡i inductive bias máº¡nh, do Ä‘Ã³ cho phÃ©p mÃ´ hÃ¬nh khÃ¡i quÃ¡t hÃ³a tá»‘t hÆ¡n khi nÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i nhiá»u dá»¯ liá»‡u hÆ¡n. LÃ­ do Ä‘Æ¡n giáº£n bá»Ÿi vÃ¬ **kiáº¿n trÃºc Transformer khÃ´ng Ä‘áº·t ra cÃ¡c giáº£ Ä‘á»‹nh vá» Ä‘áº§u vÃ o cá»§a mÃ´ hÃ¬nh**, mÃ  nÃ³ sáº½ há»c thÃ´ng qua cÆ¡ cháº¿ attention Ä‘á»ƒ biáº¿t cÃ¡c Ä‘áº§u vÃ o khÃ¡c nhau á»Ÿ cÃ¡c vá»‹ trÃ­ khÃ¡c nhau tÆ°Æ¡ng tÃ¡c nhÆ° tháº¿ nÃ o.

# The Universal Approximation Theorem
CÃ³ má»™t bÃ i viáº¿t hay nÃ³i vá» chá»§ Ä‘á» nÃ y mÃ  má»i ngÆ°á»i cÃ³ thá»ƒ theo dÃµi thÃªm, mÃ¬nh Ä‘á»ƒ link á»Ÿ [Ä‘Ã¢y](https://medium.com/analytics-vidhya/you-dont-understand-neural-networks-until-you-understand-the-universal-approximation-theorem-85b3e7677126) nha. 

![neural_network](/assets/img/blog2/neuralnetwork.png)

NÃ³i Ä‘Æ¡n giáº£n thÃ¬ Ä‘á»‹nh lÃ­ nÃ y cho ráº±ng má»™t sá»‘ lÆ°á»£ng Ä‘áº¿m Ä‘Æ°á»£c cÃ¡c neuron trong cÃ¡i máº¡ng neuron cÃ³ thá»ƒ xáº¥p xá»‰ báº¥t kÃ¬ hÃ m liÃªn tá»¥c nÃ o vá»›i sá»± chÃ­nh xÃ¡c á»Ÿ má»™t má»©c Ä‘á»™ nÃ o Ä‘Ã³ (cháº¥p nháº­n sai sá»‘) vá»›i má»™t hÃ m kÃ­ch hoáº¡t nhÆ° Sigmoid hay ReLU hay má»™t hÃ m  nÃ o khÃ¡c.  

# CÃ¡ch mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng

TrÆ°á»›c khi tháº£o luáº­n thÃªm, trong bÃ i bÃ¡o gá»‘c, ngÆ°á»i ta cÃ³ dÃ¹ng 2 tá»« mÃ  láº§n Ä‘áº§u Ä‘á»c mÃ¬nh cÅ©ng chÆ°a hiá»ƒu rÃµ nhá»¯ng cÃ¡i Ä‘Ã³ lÃ  gÃ¬, Ä‘á»ƒ dá»… cáº¯t nghÄ©a hÆ¡n thÃ¬ á»Ÿ Ä‘Ã¢y mÃ¬nh giáº£i thÃ­ch lun:

- spatial: Má»i ngÆ°á»i cá»© hiá»ƒu lÃºc mÃ  nÃ³i cÃ¡i axis = 'spatial', tá»©c lÃ  ngÆ°á»i ta Ä‘ang Ä‘á» cáº­p Ä‘áº¿n khÃ´ng gian dÃ²ng trong cÃ¡i ma tráº­n.

- channel: ThÆ°á»ng má»i ngÆ°á»i nghe cÃ¡i channel nÃ y trong máº¥y cÃ¡i dáº¡ng bÃ i liÃªn quan Ä‘áº¿n áº£nh lÃ  nhiá»u, trong NLP, mÃ  cá»¥ thá»ƒ trong bÃ i nÃ y, khi nháº¯c tá»›i axis = 'channel', tá»©c lÃ  ngÆ°á»i ta Ä‘ang Ä‘á» cáº­p Ä‘áº¿n khÃ´ng gian cá»™t trong cÃ¡i ma tráº­n.

Láº¥y vÃ­ dá»¥ nhÆ° cÃ¡i cÃ¢u cá»§a mÃ¬nh Ä‘ang Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t ma tráº­n cÃ³ 50 dÃ²ng vÃ  512 cá»™t Ä‘i ha, thÃ¬ cÃ³ nghÄ©a cÃ¡i axis = 'spatial' sáº½ lÃ  cÃ¡i trá»¥c liÃªn quan Ä‘áº¿n khÃ´ng gian dÃ²ng, tá»©c lÃ  liÃªn quan Ä‘áº¿n sá»‘ 50, cÃ²n náº¿u axis = 'channel' sáº½ lÃ  cÃ¡i trá»¥c liÃªn quan Ä‘áº¿n khÃ´ng gian cá»™t, tá»©c liÃªn quan Ä‘áº¿n con sá»‘ 512, trong trÆ°á»ng há»£p mÃ¬nh vá»«a nÃªu, tá»©c lÃ  mÃ¬nh Ä‘ang cÃ³ 512 channel.

DÆ°á»›i Ä‘Ã¢y lÃ  cáº¥u trÃºc mÃ´ hÃ¬nh, mÃ¬nh sáº½ phÃ¢n tÃ­ch cá»¥ thá»ƒ tá»«ng thÃ nh pháº§n sau:

![gmlp_scheme](/assets/img/blog2/gmlp.png)

MÃ´ hÃ¬nh gMLP nÃ y khÃ¡ lÃ  cá»¥ thá»ƒ, nhÆ° hÃ¬nh trÃªn, má»i ngÆ°á»i cÃ³ thá»ƒ tháº¥y ráº±ng mÃ´ hÃ¬nh sáº½ bao gá»“m má»™t cÃ¡i chá»“ng gá»“m $L$ khá»‘i Ä‘Ã¨ lÃªn nhau vá»›i kÃ­ch thÆ°á»›c vÃ  cáº¥u trÃºc nhÆ° nhau. 

Biá»ƒu diá»…n Ä‘áº§u vÃ o (a.k.a input embeddings) sáº½ lÃ  má»™t ma tráº­n cÃ³ kÃ­ch thÆ°á»›c  $x \in \mathbb{R}^{n \times d}$ vá»›i $n$ lÃ  Ä‘á»™ dÃ i cá»§a cÃ¢u vÃ  $d$ lÃ  chiá»u cá»§a vector biá»ƒu diá»…n. 

Sau Ä‘Ã³ ma tráº­n biá»ƒu diá»…n nÃ y sáº½ Ä‘Æ°á»£c chuáº©n hÃ³a (cÃ¡ch thá»©c chuáº©n hÃ³a sáº½ lÃ  chuáº©n hÃ³a theo axis = 'channel'). Chuáº©n hÃ³a á»Ÿ Ä‘Ã¢y sáº½ lÃ  chuáº©n hÃ³a theo layer, náº¿u ai chÆ°a sá»­ dá»¥ng cÃ¡i nÃ y trong pytorch thÃ¬ nÃ³ lÃ  `torch.nn.LayerNorm` nha má»i ngÆ°á»i, vá» cÆ¡ báº£n thÃ¬ cÃ´ng tá»©c toÃ¡n cá»§a cÃ¡i nÃ y nhÆ° sau:

$$
y = \frac{{x - E(x)}}{{\sqrt{{Var(x) + \epsilon}}}} \cdot \gamma + \beta
$$

Trong Ä‘Ã³ cÃ¡i mean vá»›i cÃ¡i std thÃ¬ nÃ³ láº¥y tá»« D chiá»u ()

VÃ  káº¿ Ä‘áº¿n sau Ä‘Ã³, ma tráº­n Ä‘Æ°á»£c tráº£ ra tá»« bÆ°á»›c chuáº©n hÃ³a sáº½ Ä‘Æ°á»£c chiáº¿u qua má»™t khÃ´ng gian khÃ¡c. BÆ°á»›c chiáº¿u nÃ y khÃ¡ Ä‘Æ¡n giáº£n thÃ´i má»i ngÆ°á»i, cá»© tÆ°á»Ÿng tÆ°á»£ng nÃ³ lÃ  má»™t hÃ m toÃ¡n nhÆ° kiá»ƒu $f: \mathbb{R}^{150 \times 512} \rightarrow \mathbb{R}^{150 \times 256}$, hiá»ƒu nhÆ° váº­y thÃ¬ nÃ³ chÃ­nh lÃ  phÃ©p `nn.Linear()` trong pytorch luÃ´n. 

Sau Ä‘Ã³ ma tráº­n má»›i nÃ y sáº½ Ä‘i qua má»™t hÃ m kÃ­ch hoáº¡t nÃ o Ä‘Ã³, cÃ³ thá»ƒ lÃ  hÃ m ReLU hay GeLU hay gÃ¬ gÃ¬ Ä‘áº¥y, nhÆ° váº­y thÃ¬ ta cÃ³ thá»ƒ biá»ƒu diá»…n dÆ°á»›i cÃ´ng thá»©c toÃ¡n há»c tá»« sau bÆ°á»›c normalize nhÆ° sau:

$$
Z = \sigma(XU)
$$

Vá»›i X lÃ  Ä‘áº§u vÃ o, U lÃ  má»™t ma tráº­n cho phÃ©p Ã¡nh xáº¡ X sang má»™t khÃ´ng gian cÃ³ sá»‘ chiá»u khÃ¡c, cÃ²n $\sigma$ lÃ  má»™t hÃ m kÃ­ch hoáº¡t phi tuyáº¿n nÃ o Ä‘Ã³ nhÆ° nÃ£y mÃ¬nh nÃ³i.  

# á»¨ng dá»¥ng

# PhÃ¢n tÃ­ch Æ°u vÃ  nhÆ°á»£c Ä‘iá»ƒm

# Káº¿t luáº­n

# Tháº£o luáº­n thÃªm