---
title: 'Paper Explained 2: Pay Attention to MLPs'
date: 2024-02-02
permalink: /posts/2024/02/blog-post-2/
tags:
  - Paper explained
  - Deep Learning
published: true
---

ğŸ“£ Attention Attention Attention, attention nÃ y attention kia, quÃ¡ nhiá»u attention. ğŸ˜… Trong bÃ i viáº¿t nÃ y chÃºng ta sáº½ cÃ¹ng tháº£o luáº­n vá» má»™t cÃ¡ch khÃ¡c cÅ©ng Ä‘Ã¡ng nháº­n Ä‘Æ°á»£c attention, máº·c dÃ¹ khÃ´ng pháº£i attention. ğŸ¤” Chá»§ Ä‘á» hÃ´m nÃ y cá»§a chÃºng ta sáº½ bÃ n vá» MLP (cá»¥ thá»ƒ hÆ¡n thÃ¬ lÃ  má»™t biáº¿n thá»ƒ cá»§a máº¡ng MLP truyá»n thá»‘ng) nÃ³i chung vÃ  gMLP nÃ³i riÃªng (Highlight cá»§a biáº¿n thá»ƒ nÃ y lÃ  Spatial Gating Unit - Má»™t Ä‘Æ¡n vá»‹ Ä‘á»ƒ kiá»ƒm soÃ¡t thÃ´ng tin). ğŸ¯ BÃ i nÃ y khÃ¡ hay, cÃ¹ng Ä‘á»c nháº¿ !!! ğŸ“šğŸ‰

# Giá»›i thiá»‡u.
**!! NOTICE !!** Chá»¯ kiáº¿n trÃºc vá»›i chá»¯ mÃ´ hÃ¬nh nÃ³ hÆ¡i nháº¡y cáº£m má»i ngÆ°á»i, nÃªn trong bÃ i nÃ y, mÃ¬nh nÃ³i kiáº¿n trÃºc lÃ  nÃ³i chung, cÃ²n mÃ´ hÃ¬nh lÃ  nÃ³i cá»¥ thá»ƒ, chá»‰ Ä‘Ã­ch danh mÃ´ hÃ¬nh Ä‘Ã³ luÃ´n. z thui, cheers

CÃ¡c báº¡n mÃ  cÃ³ hay cáº­p nháº­t cÃ¡c thÃ´ng tin vá» máº¥y cÃ¡i mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) nhÆ° máº¥y cÃ¡i Mistral, LLaMa, v.v... thÃ¬ cháº¯c cÃ¡c báº¡n cÅ©ng Ä‘Ã£ biáº¿t cÃ¡i pháº§n cá»‘t lÃµi cá»§a máº¥y mÃ´ hÃ¬nh khá»§ng nÃ y lÃ  kiáº¿n trÃºc Transformer (mÃ  cá»¥ thá»ƒ hÆ¡n lÃ  cÆ¡ cháº¿ attention cá»§a nÃ³). 

Nhá»¯ng cÃ¡i kiáº¿n trÃºc cá»§a Transformer bao gá»“m 2 thÃ nh pháº§n chÃ­nh lÃ  Ä‘Ã³ lÃ  má»™t cÃ¡i **kiáº¿n trÃºc recurrent-free** (do Ä‘Ã³ mÃ  nÃ³ cho phÃ©p tÃ­nh cÃ¡i representation cá»§a cÃ¡c tokens má»™t cÃ¡ch song song) vÃ  khá»‘i **multi-head self-attention** cho phÃ©p Ä‘a dáº¡ng vÃ  tá»•ng há»£p thÃ´ng tin giá»¯a cÃ¡c token vá»›i nhau. 

Váº­y cÃ¢u há»i Ä‘áº·t ra á»Ÿ Ä‘Ã¢y lÃ : "**CÃ¡i khá»‘i attention Ä‘Ã³ cÃ³ cáº§n thiáº¿t khÃ´ng?**". Bá»Ÿi vÃ¬ cÃ¡i cÆ¡ cháº¿ attention giá»›i thiá»‡u má»™t cÃ¡i gá»i lÃ  "ThiÃªn kiáº¿n quy náº¡p" (Inductive bias) (CÃ¡i inductive bias hiá»ƒu theo nghÄ©a khÃ¡c lÃ  kháº£ nÄƒng generalize cá»§a mÃ´ hÃ¬nh tá»›i Ä‘Ã¢u Ä‘á»‘i vá»›i cÃ¡c giáº£ thuyáº¿t khÃ¡c nhau) vÃ  cÃ¡i inductive bias nÃ y sáº½ tá»‘t khi mÃ  cÃ¡c **sá»± tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c tokens trong cÃ¢u nÃªn Ä‘Æ°á»£c tham sá»‘ hÃ³a má»™t cÃ¡ch dynamic dá»±a trÃªn biá»ƒu diá»…n Ä‘áº§u vÃ o** (pháº§n dÆ°á»›i mÃ¬nh cÃ³ giáº£i thÃ­ch thÃªm vá» inductive bias), tuy nhiÃªn khi nhÃ¬n theo má»™t cÃ¡i gÃ³c nhÃ¬n khÃ¡c thÃ¬ nhá»¯ng **cáº¥u trÃºc MLP vá»›i cÃ¡c tham sá»‘ tÄ©nh cho phÃ©p nÃ³ biá»ƒu diá»…n má»™t hÃ m báº¥t ká»³ nÃ o Ä‘Ã³** (theo nhÆ° Universal Approximation Theorem - Ä‘áº¡i khÃ¡i cÃ¡i theorem nÃ y cho ráº±ng cÃ¡c FFN (Feed-forward network) cÃ³ thá»ƒ xáº¥p xá»‰ báº¥t cá»© hÃ m liÃªn tá»¥c nÃ o báº±ng).   

Äá»ƒ tráº£ lá»i cÃ¢u há»i á»Ÿ trÃªn, tÃ¡c giáº£ má»›i giá»›i thiá»‡u má»™t mÃ´ hÃ¬nh má»›i gá»i lÃ  gMLP bá»Ÿi vÃ¬ nÃ³ Ä‘Æ°á»£c táº¡o ra tá»« kiáº¿n trÃºc MLP vÃ  má»™t Ä‘Æ¡n vá»‹ cá»•ng (gating unit). 

VÃ  spoil trÆ°á»›c, nÃ³ hiá»‡u quáº£.

Pháº§n nÃ y sáº½ dÃ¹ng Ä‘á»ƒ giáº£i thÃ­ch cÃ¡c khÃ¡i niá»‡m liÃªn quan mÃ  nÃ³ Ã­t phá»• biáº¿n hÆ¡n. Náº¿u á»Ÿ trÃªn má»i ngÆ°á»i Ä‘Ã£ hiá»ƒu rá»“i thÃ¬ cá»© máº¡nh dáº¡ng xuá»‘gn pháº§n mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng luÃ´n cÅ©ng Ä‘Æ°á»£c, cÃ²n náº¿u mÃ  nhá»¯ng gÃ¬ mÃ¬nh nÃ³i á»Ÿ trÃªn chÆ°a cá»¥ thá»ƒ thÃ¬ pháº§n nÃ y sáº½ giáº£i thÃ­ch kÄ© hÆ¡n Ä‘á»ƒ má»i ngÆ°á»i cÃ³ thá»ƒ hiá»ƒu vá» cÃ¡ch mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng. CÃ³ má»™t bÃ i giáº£i thÃ­ch ráº¥t hay vÃ  rÃµ rÃ ng, mÃ¬nh sáº½ Ä‘á»ƒ trong [link](https://towardsdatascience.com/a-fairy-tale-of-the-inductive-bias-d418fc61726c) nÃ y nha.

# Inductive bias (ThiÃªn kiáº¿n quy náº¡p)
Äá»ƒ dá»… hiá»ƒu hÆ¡n thÃ¬ má»i ngÆ°á»i tÆ°á»Ÿng tÆ°á»£ng: TrÆ°á»›c tá»›i giá» má»i ngÆ°á»i chá»‰ tháº¥y má»™t Ä‘Ã n thiÃªn nga Ä‘ang bÆ¡i trong má»™t cÃ¡i há»“ gáº§n nhÃ  thÃ´i, vÃ  Ä‘Ã¢y lÃ  nÆ¡i duy nháº¥t trong Ä‘á»i mÃ  má»i ngÆ°á»i cÃ³ thá»ƒ quan sÃ¡t máº¥y con thiÃªn nga nÃ y. Má»™t Ä‘á»‘ng giáº£ thuyáº¿t vá» máº¥y con thiÃªn nga nÃ y mÃ  cÃ¡c báº¡n cÃ³ thá»ƒ Ä‘áº·t ra nhÆ° sau: "ThiÃªn nga lÃ  loÃ i cÃ³ mÃ u tráº¯ng", "ThiÃªn nga chá»‰ biáº¿t bÆ¡i", "ThiÃªn nga khÃ´ng biáº¿t bay", "ThiÃªn nga Ä‘en khÃ´ng tá»“n táº¡i",v.v... NÃ³i chung lÃ  má»i ngÆ°á»i cÃ³ Ä‘Æ°a ra giáº£ thuyáº¿t nÃ o cÅ©ng Ä‘Æ°á»£c, do Ä‘Ã³ mÃ  cÃ³ vÃ´ sá»‘ giáº£ thuyáº¿t cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Æ°a ra, Ä‘Ãºng hay sai lÃ  chuyá»‡n khÃ¡c. 

![swan](/assets/img/blog2/whitevsblack.png)

Theo lÃ½ thuyáº¿t, khÃ´ng gian giáº£ thuyáº¿t lÃ  vÃ´ táº­n (tá»©c lÃ  má»i ngÆ°á»i nghÄ© ra bao nhiÃªu cÃ¡i giáº£ thuyáº¿t cÅ©ng Ä‘Æ°á»£c, khÃ´ng giá»›i háº¡n, Ä‘Ãºng sai bÃ n sau). CÃ¡i inductive bias cÃ³ thá»ƒ Ä‘Æ°á»£c hiá»ƒu nhÆ° lÃ  cÃ¡c giáº£ thuyáº¿t Ä‘Æ°á»£c Æ°u tiÃªn hÆ¡n trong khÃ´ng gian giáº£ thuyáº¿t. Láº¥y vÃ­ dá»¥ nhÆ° máº¥y bÃ i toÃ¡n nhÆ° há»“i quy tuyáº¿n tÃ­nh, má»i ngÆ°á»i Ä‘ang giáº£ Ä‘á»‹nh tá»“n táº¡i má»‘i quan há»‡ tuyáº¿n tÃ­nh giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u, vÃ  báº±ng cÃ¡i giáº£ Ä‘á»‹nh nÃ y, má»i ngÆ°á»i Ä‘á»“ng thá»i giá»›i háº¡n khÃ´ng gian giáº£ thuyáº¿t xuá»‘ng cÃ²n má»‘i quan há»‡ tuyáº¿n tÃ­nh. 

![inductive_bias](/assets/img/blog2/inductive_bias.png)

Váº­y trong khuÃ´n khá»• cá»§a machine learning thÃ¬ Ä‘iá»u nÃ y lÃ  sao? Má»i ngÆ°á»i Ä‘ang cÃ³ Ä‘a dáº¡ng dá»¯ liá»‡u (áº£nh, chá»¯, tÃ­n hiá»‡u, v.v...) vÃ  vÃ´ vÃ n loáº¡i mÃ´ hÃ¬nh khÃ¡c nhau (tÃ­ch cháº­p, há»“i quy, v.v...) vÃ  má»—i mÃ´ hÃ¬nh khÃ¡c nhau nÃ y Ä‘á»u cÃ³ má»™t cÃ¡i inductive bias Ä‘á»ƒ nÃ³ hoáº¡t Ä‘á»™ng tá»‘t khÃ¡c nhau. VÃ­ dá»¥ nhÆ° lÃ  cÃ¡c máº¡ng CNN Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn giáº£ thuyáº¿t cÃ¡c Ä‘iá»ƒm pixel náº±m gáº§n nhau thÃ¬ cÃ³ liÃªn quan tá»›i nhau vÃ  mÃ´ hÃ¬nh nÃªn há»c Ä‘Æ°á»£c cÃ¡i sá»± liÃªn quan nÃ y. 

VÃ  nhÃ¢n váº­t chÃ­nh cá»§a chÃºng ta lÃ  kiáº¿n trÃºc Transformer, mÃ´ hÃ¬nh nÃ y khÃ´ng cÃ³ má»™t cÃ¡i inductive bias máº¡nh, do Ä‘Ã³ cho phÃ©p mÃ´ hÃ¬nh khÃ¡i quÃ¡t hÃ³a tá»‘t hÆ¡n khi nÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i nhiá»u dá»¯ liá»‡u hÆ¡n. LÃ­ do Ä‘Æ¡n giáº£n bá»Ÿi vÃ¬ **kiáº¿n trÃºc Transformer khÃ´ng Ä‘áº·t ra cÃ¡c giáº£ Ä‘á»‹nh vá» Ä‘áº§u vÃ o cá»§a mÃ´ hÃ¬nh**, mÃ  nÃ³ sáº½ há»c thÃ´ng qua cÆ¡ cháº¿ attention Ä‘á»ƒ biáº¿t cÃ¡c Ä‘áº§u vÃ o khÃ¡c nhau á»Ÿ cÃ¡c vá»‹ trÃ­ khÃ¡c nhau tÆ°Æ¡ng tÃ¡c nhÆ° tháº¿ nÃ o.

# The Universal Approximation Theorem
NÃ³i theo Ã½ nghÄ©a toÃ¡n thÃ¬ cÃ¡c cáº¥u trÃºc máº¡ng neural nÃ o tháº­t cháº¥t cÅ©ng lÃ  Ä‘i tÃ¬m má»™t hÃ m sá»‘ $f$ sao cho $y = f(x)$ vá»›i $x$ lÃ  dá»¯ liá»‡u Ä‘áº§u vÃ o cÃ²n $y$ lÃ  dá»¯ liá»‡u Ä‘áº§u ra. Váº­y Ä‘á»‹nh lÃ½ nÃ y muá»‘n nÃ³i ráº±ng cÃ¡c máº¡ng Neural cÅ©ng cÃ³ tÃ­nh cháº¥t universality, tá»©c lÃ  báº¥t ká»ƒ hÃ m $f(x)$ ra sao, luÃ´n cÃ³ má»™t máº¡ng cÃ³ thá»ƒ xáº¥p xá»‰ káº¿t quáº£ Ä‘Ã³. 




# CÃ¡ch mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng

# á»¨ng dá»¥ng

# PhÃ¢n tÃ­ch Æ°u vÃ  nhÆ°á»£c Ä‘iá»ƒm

# Káº¿t luáº­n

# Tháº£o luáº­n thÃªm