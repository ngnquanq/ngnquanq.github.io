---
title: 'Paper Explained 2: Pay Attention to MLPs'
date: 2024-02-02
permalink: /posts/2024/02/blog-post-2/
tags:
  - Paper explained
  - Deep Learning
published: true
---

📣 Attention Attention Attention, attention này attention kia, quá nhiều attention. 😅 Trong bài viết này chúng ta sẽ cùng thảo luận về một cách khác cũng đáng nhận được attention, mặc dù không phải attention. 🤔 Chủ đề hôm này của chúng ta sẽ bàn về MLP (cụ thể hơn thì là một biến thể của mạng MLP truyền thống) nói chung và gMLP nói riêng (Highlight của biến thể này là Spatial Gating Unit - Một đơn vị để kiểm soát thông tin). 🎯 Bài này khá hay, cùng đọc nhế !!! 📚🎉

# Giới thiệu.
**!! NOTICE !!** Chữ kiến trúc với chữ mô hình nó hơi nhạy cảm mọi người, nên trong bài này, mình nói kiến trúc là nói chung, còn mô hình là nói cụ thể, chỉ đích danh mô hình đó luôn. z thui, cheers

Các bạn mà có hay cập nhật các thông tin về mấy cái mô hình ngôn ngữ lớn (LLM) như mấy cái Mistral, LLaMa, v.v... thì chắc các bạn cũng đã biết cái phần cốt lõi của mấy mô hình khủng này là kiến trúc Transformer (mà cụ thể hơn là cơ chế attention của nó). 

Những cái kiến trúc của Transformer bao gồm 2 thành phần chính là đó là một cái **kiến trúc recurrent-free** (do đó mà nó cho phép tính cái representation của các tokens một cách song song) và khối **multi-head self-attention** cho phép đa dạng và tổng hợp thông tin giữa các token với nhau. 

Vậy câu hỏi đặt ra ở đây là: "**Cái khối attention đó có cần thiết không?**". Bởi vì xét theo một mặt nào đó, cơ chế attention có một cái inductive bias đó là **tương tác giữa các token nên được tham số một cách dynamic dựa trên biểu diễn đầu vào**, tức là các cái representation có thể thay đổi dựa trên input của mô hình. Nhưng xét theo mặt khác, dựa vào Universal Approximation Theorem, bất kì cấu trúc MLP nào với cũng có thể **xấp xỉ một hàm nào đó với tham số cố định**. Và từ những suy nghĩ trên, mình có thể chuyển câu hỏi vừa đặt ra sang một câu khác cũng khá tương tự: "**Điều gì đóng góp tới thành công của mô hình hơn? Một cái inductive bias yếu trong cơ chế attention hay khả năng xấp xỉ bất kỳ hàm toán học nào của các mạng Neural?**"

Thì bài nghiên cứu của các tác giả, bên cạnh việc trả lời câu hỏi ở trên, tác giả mới giới thiệu một mô hình mới gọi là gMLP bởi vì nó được tạo ra từ kiến trúc MLP và một đơn vị cổng (gating unit). 

Và spoil trước, nó hiệu quả.

# Inductive bias (Thiên kiến quy nạp)
Để dễ hiểu hơn thì mọi người tưởng tượng: Trước tới giờ mọi người chỉ thấy một đàn thiên nga đang bơi trong một cái hồ gần nhà thôi, và đây là nơi duy nhất trong đời mà mọi người có thể quan sát mấy con thiên nga này. Một đống giả thuyết về mấy con thiên nga này mà các bạn có thể đặt ra như sau: "Thiên nga là loài có màu trắng", "Thiên nga chỉ biết bơi", "Thiên nga không biết bay", "Thiên nga đen không tồn tại",v.v... Nói chung là mọi người có đưa ra giả thuyết nào cũng được, do đó mà có vô số giả thuyết có thể được đưa ra, đúng hay sai là chuyện khác. 

![swan](/assets/img/blog2/whitevsblack.png)

Theo lý thuyết, không gian giả thuyết là vô tận (tức là mọi người nghĩ ra bao nhiêu cái giả thuyết cũng được, không giới hạn, đúng sai bàn sau). Cái inductive bias có thể được hiểu như là các giả thuyết được ưu tiên hơn trong không gian giả thuyết. Lấy ví dụ như mấy bài toán như hồi quy tuyến tính, mọi người đang giả định tồn tại mối quan hệ tuyến tính giữa các điểm dữ liệu, và bằng cái giả định này, mọi người đồng thời giới hạn không gian giả thuyết xuống còn mối quan hệ tuyến tính. 

![inductive_bias](/assets/img/blog2/inductive_bias.png)

Vậy trong khuôn khổ của machine learning thì điều này là sao? Mọi người đang có đa dạng dữ liệu (ảnh, chữ, tín hiệu, v.v...) và vô vàn loại mô hình khác nhau (tích chập, hồi quy, v.v...) và mỗi mô hình khác nhau này đều có một cái inductive bias để nó hoạt động tốt khác nhau. Ví dụ như là các mạng CNN được xây dựng dựa trên giả thuyết các điểm pixel nằm gần nhau thì có liên quan tới nhau và mô hình nên học được cái sự liên quan này. 

Và nhân vật chính của chúng ta là kiến trúc Transformer, mô hình này không có một cái inductive bias mạnh, do đó cho phép mô hình khái quát hóa tốt hơn khi nó được huấn luyện với nhiều dữ liệu hơn. Lí do đơn giản bởi vì **kiến trúc Transformer không đặt ra các giả định về đầu vào của mô hình**, mà nó sẽ học thông qua cơ chế attention để biết các đầu vào khác nhau ở các vị trí khác nhau tương tác như thế nào.

# The Universal Approximation Theorem
Có một bài viết hay nói về chủ đề này mà mọi người có thể theo dõi thêm, mình để link ở [đây](https://medium.com/analytics-vidhya/you-dont-understand-neural-networks-until-you-understand-the-universal-approximation-theorem-85b3e7677126) nha. 

![neural_network](/assets/img/blog2/neuralnetwork.png)

Nói đơn giản thì định lí này cho rằng một số lượng đếm được các neuron trong cái mạng neuron có thể xấp xỉ bất kì hàm liên tục nào với sự chính xác ở một mức độ nào đó (chấp nhận sai số) với một hàm kích hoạt như Sigmoid hay ReLU hay một hàm  nào khác.  

# Cách mô hình hoạt động
Trước khi thảo luận thêm, trong bài báo gốc, người ta có dùng 2 từ mà lần đầu đọc mình cũng chưa hiểu rõ những cái đó là gì, để dễ cắt nghĩa hơn thì ở đây mình giải thích lun:

- spatial: Mọi người cứ hiểu lúc mà nói cái axis = 'spatial', tức là người ta đang đề cập đến không gian dòng trong cái ma trận.

- channel: Thường mọi người nghe cái channel này trong mấy cái dạng bài liên quan đến ảnh là nhiều, trong NLP, mà cụ thể trong bài này, khi nhắc tới axis = 'channel', tức là người ta đang đề cập đến không gian cột trong cái ma trận.

Lấy ví dụ như cái câu của mình đang được biểu diễn dưới dạng một ma trận có 50 dòng và 512 cột đi ha, thì có nghĩa cái axis = 'spatial' sẽ là cái trục liên quan đến không gian dòng, tức là liên quan đến số 50, còn nếu axis = 'channel' sẽ là cái trục liên quan đến không gian cột, tức liên quan đến con số 512, trong trường hợp mình vừa nêu, tức là mình đang có 512 channel.

Dưới đây là cấu trúc mô hình, mình sẽ phân tích cụ thể từng thành phần sau:


![gmlp_scheme](/assets/img/blog2/gmlp.png)

Mô hình gMLP này khá là cụ thể, như hình trên, mọi người có thể thấy rằng mô hình sẽ bao gồm một cái chồng gồm $L$ khối đè lên nhau với kích thước và cấu trúc như nhau và mỗi khối sẽ đưuọc định nghĩa như sau:

$$
Z = \sigma(XU), \quad \tilde{Z} = s(Z), \quad Y = \tilde{Z}V
$$

Cái $Z$ với cái $Y$ về cơ bản là phép Channel Projection, còn cái $\tilde{Z}$ là cái Spatial Gating Unit, với điểm nhấn của nó là phép Spatial Projection. 

TODO: Giải thích cái $s(\cdot)$
## Channel Projection
Hiểu đơn giản thì channel projection chỉ là một phép chiếu thôi mọi người, nếu mọi người ít sử dụng pytorch thì nó là cái `torch.nn.Linear(in_features, out_features)` á mọi người. Công thức toán của phép này là như sau:

$$y = xA^T + b$$

Với x là ma trận đầu vào, A là ma trận giúp ánh xạ từ không gian có cái chiều đầu vào là `in_features` sang chiều `out_features`. Còn $b$ là bias

Nên là cái paper này nói cao siêu vậy chứ cái này đơn giản là cái MLP thôi à 🥹

## Spatial Gating Unit
Spatial Gating Unit, hay gọi nhanh SGU là một khối cho phép mô hình gMLP này học được tương tácc giữa các token với nhau thông qua một phép chiếu khác bên trong khối này, phép chiếu này cũng đơn giản là một phép ánh xạ tuyến tính luôn mọi người. Nhưng không đơn giản như cái channel projection, cái này phía tác giả gọi cái khối đảm nhiệm việc này là spatial projection. Bên cạnh đó tác giả cũng đề cập thêm là cái layer này sẽ chứa một cái gọi là **contraction operator** (theo mình tìm hiểu thì cái này là một một hàm ánh xạ $T: V \mapsto V$ được định nghĩa bởi $T(v) = kv$ với $k \in \mathbb{R}$ sao cho $k < 1$). Và như những gì đã đề cập về việc đây là phép biến đổi tuyến tính thì nó có công thức như sau:

$$
f_{W,b}(Z) = WZ + b
$$

Với $W \in \mathbb{R}^{n \times n}$ , trong đó n là chiều dài của câu như hồi nãy mình đề cập á mọi người. Như vậy $W$ sẽ là một ma trận vuông ánh xạ từ không gian n sang chính không gian n, còn $b$ thì là bias thôi. 

Lấy ví dụ như nếu như cái câu của mình có 150 tokens như nãy đi ha, thì ma trận $W$ sẽ có kích thước là (150, 150). Và không như cơ chế self-attention có tính inductive bias yếu, do đó mà $W(Z)$ là không phải một biểu diễn linh hoạt dựa vào đầu vào $Z$, mà ma trận ánh xạ $W$ này sẽ độc lập khỏi biểu diễn đầu vào.

Như vậy mà cái layer $S(\cdot)$ sẽ được biểu diễn dưới dạng công thức như sau:

$$s(Z) = Z \odot f_{W,b}(Z)$$

Với $\odot$ là phép nhân hamdard (ai không quen chữ này thì hiểu là phép nhân elemet-wise cũng được). 

Thì khối này về cơ bản là xong rồi nha mọi người, tuy nhiên cần phải lưu ý thêm về một vài điều chỉnh của tác giả. Để cho quá trình huấn luyện model nó ổn định hơn, thì đầu tiên các tác giả đặt giá trị khởi tạo của ma trận $W$ là các giá trị xấp xỉ giá 0, trong khi đó đặt giá trị của $b$ thẳng bằng 1, điều này cũng đồng nghĩa với việc $f_{W,b} \approx \mathbf{1}$, điều này trực tiếp dẫn tới $s(Z) \approx Z$ ở giai đoạn đầu tiên của quá trình huấn luyện, đảm bảo rằng mỗi khối gMLP sẽ hoạt động như một cái FFN (tức là trong đó mỗi token được xử lí độc lập với nhau và mối quan hệ giữa các token đó sẽ được cập nhật thông qua quá trình huấn luyện). Và ngoài ra bằng việc chia ma trận $Z$ thành 2 nửa ($Z_1, Z_2$) thì tác giả thấy làm vậy nó thuận tiện hơn (chắc là theo thực nghiệm, mình không thấy tác giả đề cập gì về vụ này 😻), à mà chia 1 nửa ở đây là chia theo channel á nha, ví dụ như đầu vào Z của mình đang là (150, 512) thì lúc nãy mỗi cái $Z_1$, $Z_2$ sẽ có cái kích thước là (150, 256). Như vậy thì cái công thức ở trên có thể ghi lại như dưới đây:

$$y = Z_1 \odot f_{W,b}(Z_2)$$

Mục đích nằm ở ma trận vuông $W$ sẽ học được các tương tác có ý nghĩa giữa các token. Để hiểu rõ hơn nó là ra sao thì mọi người nhìn thử cái hình dưới đây: 

![weight_matrix_4h](/assets/img/blog2/weight_matrix.png)

Trong lúc chạy code, mình có để cái head = 4 nên về cơ bản thì mình sẽ có 4 cái ma trận $W$ rồi có gì sau này mình gom 4 cái head vô thành 1 cái head là được. Thì về cơ bản, 4 cái head này là cố định, chứ nó không có dynamic như cái cơ chế attention.

Ngoài ra bên phía tác giả cũng có kết hợp thêm cái cơ chế attention vô trong mấy cái khối này, thì họ được kết quả như dưới đây:

![weight_attention_matrix](/assets/img/blog2/gmlp_attention_matrix.png)

## Toàn bộ cấu trúc mô hình

Biểu diễn đầu vào (a.k.a input embeddings) sẽ là một ma trận có kích thước  $x \in \mathbb{R}^{n \times d}$ với $n$ là độ dài của câu và $d$ là chiều của vector biểu diễn. 

Sau đó ma trận biểu diễn này sẽ được chuẩn hóa (cách thức chuẩn hóa sẽ là chuẩn hóa theo axis = 'channel'). Chuẩn hóa ở đây sẽ là chuẩn hóa theo layer, nếu ai chưa sử dụng cái này trong pytorch thì nó là `torch.nn.LayerNorm` nha mọi người, về cơ bản thì công tức toán của cái này như sau:

$$y = \frac{(x - E[x]) \cdot \gamma}{\sqrt{Var[x] + \epsilon}} + \beta$$

Cái này mà giải thích nhanh thì nó kiểu như là giá trị đầu vào của tất cả neuron trong cùng một layer được chuẩn hóa cho từng diểm dữ liệu.

Và kế đến sau đó, ma trận được trả ra từ bước chuẩn hóa sẽ được chiếu qua một không gian khác. Bước chiếu này khá đơn giản thôi mọi người, cứ tưởng tượng nó là một hàm toán như kiểu $f: \mathbb{R}^{150 \times 512} \rightarrow \mathbb{R}^{150 \times 256}$, hiểu như vậy thì nó chính là phép `nn.Linear()` trong pytorch luôn. 

Sau đó ma trận mới này sẽ đi qua một hàm kích hoạt nào đó, có thể là hàm ReLU hay GeLU hay gì gì đấy, như vậy thì ta có thể biểu diễn dưới công thức toán học từ sau bước normalize như sau:

$$
Z = \sigma(XU)
$$

Với X là đầu vào, U là một ma trận cho phép ánh xạ X sang một không gian có số chiều khác, còn $\sigma$ là một hàm kích hoạt phi tuyến nào đó như nãy mình nói.  

# Ứng dụng

# Phân tích ưu và nhược điểm

# Kết luận

# Thảo luận thêm